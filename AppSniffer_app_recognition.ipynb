{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cPGvjx5dEGe_",
        "outputId": "269b2cd7-bfa0-4443-82ba-f618ed56e458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autogluon\n",
            "  Downloading autogluon-0.5.2-py3-none-any.whl (9.6 kB)\n",
            "Collecting autogluon.vision==0.5.2\n",
            "  Downloading autogluon.vision-0.5.2-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting autogluon.core[all]==0.5.2\n",
            "  Downloading autogluon.core-0.5.2-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 27.8 MB/s \n",
            "\u001b[?25hCollecting autogluon.timeseries[all]==0.5.2\n",
            "  Downloading autogluon.timeseries-0.5.2-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting autogluon.multimodal==0.5.2\n",
            "  Downloading autogluon.multimodal-0.5.2-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 67.2 MB/s \n",
            "\u001b[?25hCollecting autogluon.features==0.5.2\n",
            "  Downloading autogluon.features-0.5.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting autogluon.tabular[all]==0.5.2\n",
            "  Downloading autogluon.tabular-0.5.2-py3-none-any.whl (274 kB)\n",
            "\u001b[K     |████████████████████████████████| 274 kB 60.7 MB/s \n",
            "\u001b[?25hCollecting autogluon.text==0.5.2\n",
            "  Downloading autogluon.text-0.5.2-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 493 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.5.2->autogluon) (3.2.2)\n",
            "Collecting dask<=2021.11.2,>=2021.09.1\n",
            "  Downloading dask-2021.11.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.1,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.5.2->autogluon) (1.0.2)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.5.2->autogluon) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.5.2->autogluon) (2.23.0)\n",
            "Requirement already satisfied: pandas!=1.4.0,<1.5,>=1.2.5 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.5.2->autogluon) (1.3.5)\n",
            "Collecting autogluon.common==0.5.2\n",
            "  Downloading autogluon.common-0.5.2-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: scipy<1.8.0,>=1.5.4 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.5.2->autogluon) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23,>=1.21 in /usr/local/lib/python3.7/dist-packages (from autogluon.core[all]==0.5.2->autogluon) (1.21.6)\n",
            "Collecting distributed<=2021.11.2,>=2021.09.1\n",
            "  Downloading distributed-2021.11.2-py3-none-any.whl (802 kB)\n",
            "\u001b[K     |████████████████████████████████| 802 kB 49.8 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.25.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 52.9 MB/s \n",
            "\u001b[?25hCollecting hyperopt<0.2.8,>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 18.0 MB/s \n",
            "\u001b[?25hCollecting ray<1.14,>=1.13\n",
            "  Downloading ray-1.13.0-cp37-cp37m-manylinux2014_x86_64.whl (54.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.5 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting psutil<6,>=5.7.3\n",
            "  Downloading psutil-5.9.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 62.6 MB/s \n",
            "\u001b[?25hCollecting Pillow<9.1.0,>=9.0.1\n",
            "  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 53.6 MB/s \n",
            "\u001b[?25hCollecting omegaconf<2.2.0,>=2.1.1\n",
            "  Downloading omegaconf-2.1.2-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.18.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.multimodal==0.5.2->autogluon) (3.17.3)\n",
            "Collecting transformers<4.21.0,>=4.18.0\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 58.5 MB/s \n",
            "\u001b[?25hCollecting pytorch-metric-learning<1.4.0,>=1.3.0\n",
            "  Downloading pytorch_metric_learning-1.3.2-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 67.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchtext<0.14.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.multimodal==0.5.2->autogluon) (0.13.1)\n",
            "Collecting timm<0.6.0\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 69.8 MB/s \n",
            "\u001b[?25hCollecting nptyping<1.5.0,>=1.4.4\n",
            "  Downloading nptyping-1.4.4-py3-none-any.whl (31 kB)\n",
            "Collecting torchmetrics<0.8.0,>=0.7.2\n",
            "  Downloading torchmetrics-0.7.3-py3-none-any.whl (398 kB)\n",
            "\u001b[K     |████████████████████████████████| 398 kB 70.5 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning<1.7.0,>=1.6.0\n",
            "  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
            "\u001b[K     |████████████████████████████████| 585 kB 66.9 MB/s \n",
            "\u001b[?25hCollecting nlpaug<=1.1.10,>=1.1.10\n",
            "  Downloading nlpaug-1.1.10-py3-none-any.whl (410 kB)\n",
            "\u001b[K     |████████████████████████████████| 410 kB 71.5 MB/s \n",
            "\u001b[?25hCollecting fairscale<=0.4.6,>=0.4.5\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 69.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch<1.13,>=1.9 in /usr/local/lib/python3.7/dist-packages (from autogluon.multimodal==0.5.2->autogluon) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision<0.14.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.multimodal==0.5.2->autogluon) (0.13.1+cu113)\n",
            "Requirement already satisfied: smart-open<5.3.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.multimodal==0.5.2->autogluon) (5.2.1)\n",
            "Collecting sentencepiece<0.2.0,>=0.1.95\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 53.6 MB/s \n",
            "\u001b[?25hCollecting scikit-image<0.20.0,>=0.19.1\n",
            "  Downloading scikit_image-0.19.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk<4.0.0,>=3.4.5 in /usr/local/lib/python3.7/dist-packages (from autogluon.multimodal==0.5.2->autogluon) (3.7)\n",
            "Requirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.5.2->autogluon) (2.6.3)\n",
            "Collecting xgboost<1.5,>=1.4\n",
            "  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 166.7 MB 18 kB/s \n",
            "\u001b[?25hCollecting lightgbm<3.4,>=3.3\n",
            "  Downloading lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 59.9 MB/s \n",
            "\u001b[?25hCollecting catboost<1.1,>=1.0\n",
            "  Downloading catboost-1.0.6-cp37-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.6 MB 115 kB/s \n",
            "\u001b[?25hRequirement already satisfied: fastai<2.8,>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.5.2->autogluon) (2.7.9)\n",
            "Collecting autogluon-contrib-nlp==0.0.1b20220208\n",
            "  Downloading autogluon_contrib_nlp-0.0.1b20220208-py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 67.2 MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses>=0.0.38\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 59.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers>=0.9.4\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 55.8 MB/s \n",
            "\u001b[?25hCollecting flake8\n",
            "  Downloading flake8-5.0.4-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 539 kB/s \n",
            "\u001b[?25hCollecting sentencepiece<0.2.0,>=0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.9 MB/s \n",
            "\u001b[?25hCollecting contextvars\n",
            "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.5.2->autogluon) (6.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.5.2->autogluon) (2022.6.2)\n",
            "Collecting psutil<6,>=5.7.3\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 54.9 MB/s \n",
            "\u001b[?25hCollecting gluonts<0.10.0,>=0.8.0\n",
            "  Downloading gluonts-0.9.9-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 53.3 MB/s \n",
            "\u001b[?25hCollecting sktime~=0.11.4\n",
            "  Downloading sktime-0.11.4-py3-none-any.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 13.9 MB/s \n",
            "\u001b[?25hCollecting tbats~=1.1\n",
            "  Downloading tbats-1.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 746 kB/s \n",
            "\u001b[?25hCollecting pmdarima~=1.8.2\n",
            "  Downloading pmdarima-1.8.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 51.9 MB/s \n",
            "\u001b[?25hCollecting gluoncv<0.10.6,>=0.10.5\n",
            "  Downloading gluoncv-0.10.5.post0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost<1.1,>=1.0->autogluon.tabular[all]==0.5.2->autogluon) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost<1.1,>=1.0->autogluon.tabular[all]==0.5.2->autogluon) (5.5.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost<1.1,>=1.0->autogluon.tabular[all]==0.5.2->autogluon) (0.10.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (6.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (0.12.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (21.3)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (2022.8.2)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (1.5.0)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (5.1.1)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (1.0.4)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (7.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (2.11.3)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (2.4.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (2.2.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (1.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (57.4.0)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (3.4.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (21.1.3)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (1.0.3)\n",
            "Requirement already satisfied: fastcore<1.6,>=1.4.5 in /usr/local/lib/python3.7/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (1.5.27)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (0.0.7)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.6,>=0.10.5->autogluon.vision==0.5.2->autogluon) (4.6.0.66)\n",
            "Collecting autocfg\n",
            "  Downloading autocfg-0.0.8-py3-none-any.whl (13 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: holidays>=0.9 in /usr/local/lib/python3.7/dist-packages (from gluonts<0.10.0,>=0.8.0->autogluon.timeseries[all]==0.5.2->autogluon) (0.16)\n",
            "Requirement already satisfied: pydantic~=1.1 in /usr/local/lib/python3.7/dist-packages (from gluonts<0.10.0,>=0.8.0->autogluon.timeseries[all]==0.5.2->autogluon) (1.9.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.7/dist-packages (from gluonts<0.10.0,>=0.8.0->autogluon.timeseries[all]==0.5.2->autogluon) (4.1.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from holidays>=0.9->gluonts<0.10.0,>=0.8.0->autogluon.timeseries[all]==0.5.2->autogluon) (2.8.2)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.7/dist-packages (from holidays>=0.9->gluonts<0.10.0,>=0.8.0->autogluon.timeseries[all]==0.5.2->autogluon) (2.2.4)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.7/dist-packages (from holidays>=0.9->gluonts<0.10.0,>=0.8.0->autogluon.timeseries[all]==0.5.2->autogluon) (0.3.1)\n",
            "Requirement already satisfied: convertdate>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from holidays>=0.9->gluonts<0.10.0,>=0.8.0->autogluon.timeseries[all]==0.5.2->autogluon) (2.4.0)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from convertdate>=2.3.0->holidays>=0.9->gluonts<0.10.0,>=0.8.0->autogluon.timeseries[all]==0.5.2->autogluon) (0.5.11)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 73.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.5.2->autogluon) (0.16.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm<3.4,>=3.3->autogluon.tabular[all]==0.5.2->autogluon) (0.37.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core[all]==0.5.2->autogluon) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core[all]==0.5.2->autogluon) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core[all]==0.5.2->autogluon) (3.0.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==0.5.2->autogluon) (1.2.0)\n",
            "Collecting typish>=1.7.0\n",
            "  Downloading typish-1.9.3-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 65.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.4.0,<1.5,>=1.2.5->autogluon.core[all]==0.5.2->autogluon) (2022.4)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (1.0.0)\n",
            "Requirement already satisfied: statsmodels!=0.12.0,>=0.11 in /usr/local/lib/python3.7/dist-packages (from pmdarima~=1.8.2->autogluon.timeseries[all]==0.5.2->autogluon) (0.12.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from pmdarima~=1.8.2->autogluon.timeseries[all]==0.5.2->autogluon) (1.24.3)\n",
            "Requirement already satisfied: Cython!=0.29.18,>=0.29 in /usr/local/lib/python3.7/dist-packages (from pmdarima~=1.8.2->autogluon.timeseries[all]==0.5.2->autogluon) (0.29.32)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (2.9.1)\n",
            "Collecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (3.8.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (22.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=0.6.0->dask<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (1.8.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray<1.14,>=1.13->autogluon.core[all]==0.5.2->autogluon) (4.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray<1.14,>=1.13->autogluon.core[all]==0.5.2->autogluon) (3.8.0)\n",
            "Collecting grpcio<=1.43.0,>=1.28.1\n",
            "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 47.7 MB/s \n",
            "\u001b[?25hCollecting virtualenv\n",
            "  Downloading virtualenv-20.16.5-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray<1.14,>=1.13->autogluon.core[all]==0.5.2->autogluon) (0.8.10)\n",
            "Collecting tensorboardX>=1.9\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 70.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.5.2->autogluon) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.5.2->autogluon) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core[all]==0.5.2->autogluon) (3.0.4)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.5.2->autogluon) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.5.2->autogluon) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.5.2->autogluon) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.1,>=1.0.0->autogluon.core[all]==0.5.2->autogluon) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.7/dist-packages (from sktime~=0.11.4->autogluon.timeseries[all]==0.5.2->autogluon) (0.56.3)\n",
            "Collecting deprecated>=1.2.13\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.13->sktime~=0.11.4->autogluon.timeseries[all]==0.5.2->autogluon) (1.14.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime~=0.11.4->autogluon.timeseries[all]==0.5.2->autogluon) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->sktime~=0.11.4->autogluon.timeseries[all]==0.5.2->autogluon) (4.13.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (2.4.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (3.0.10)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (0.4.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (8.1.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (0.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (3.9.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels!=0.12.0,>=0.11->pmdarima~=1.8.2->autogluon.timeseries[all]==0.5.2->autogluon) (0.5.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.7.0,>=1.6.0->autogluon.multimodal==0.5.2->autogluon) (3.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.5.2->autogluon) (0.7.8)\n",
            "Collecting tokenizers>=0.9.4\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 48.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 71.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (1.0.1)\n",
            "Collecting botocore<1.29.0,>=1.28.0\n",
            "  Downloading botocore-1.28.0-py3-none-any.whl (9.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 47.1 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 75.4 MB/s \n",
            "\u001b[?25hCollecting immutables>=0.9\n",
            "  Downloading immutables-0.19-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 72.2 MB/s \n",
            "\u001b[?25hCollecting pycodestyle<2.10.0,>=2.9.0\n",
            "  Downloading pycodestyle-2.9.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 265 kB/s \n",
            "\u001b[?25hCollecting flake8\n",
            "  Downloading flake8-5.0.3-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 510 kB/s \n",
            "\u001b[?25h  Downloading flake8-5.0.2-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 547 kB/s \n",
            "\u001b[?25h  Downloading flake8-5.0.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 393 kB/s \n",
            "\u001b[?25h  Downloading flake8-5.0.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 344 kB/s \n",
            "\u001b[?25h  Downloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting pycodestyle<2.9.0,>=2.8.0\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.0 MB/s \n",
            "\u001b[?25hCollecting pyflakes<2.5.0,>=2.4.0\n",
            "  Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting flake8\n",
            "  Downloading flake8-4.0.0-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n",
            "\u001b[?25h  Downloading flake8-3.9.2-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting pycodestyle<2.8.0,>=2.7.0\n",
            "  Downloading pycodestyle-2.7.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 691 kB/s \n",
            "\u001b[?25hCollecting pyflakes<2.4.0,>=2.3.0\n",
            "  Downloading pyflakes-2.3.1-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed<=2021.11.2,>=2021.09.1->autogluon.core[all]==0.5.2->autogluon) (2.0.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray<1.14,>=1.13->autogluon.core[all]==0.5.2->autogluon) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray<1.14,>=1.13->autogluon.core[all]==0.5.2->autogluon) (5.10.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost<1.1,>=1.0->autogluon.tabular[all]==0.5.2->autogluon) (8.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20220208->autogluon.text==0.5.2->autogluon) (4.9.1)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting distlib<1,>=0.3.5\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 63.9 MB/s \n",
            "\u001b[?25hCollecting platformdirs<3,>=2.4\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: fairscale, antlr4-python3-runtime, sacremoses, contextvars\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307252 sha256=24de1546e464dc820b4a3ea31d61d8fd4c45138fd36a1f0daebb43b922787c74\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=bc831dd73392c1e700d1c3ae33e7edce9bb616987b5255bb719dec0bec7ab7c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=5f0212681a2fda2878a6e1fa7ce14c246b8e88cc43b0dfa9544199f7396ce708\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7681 sha256=3e72777054569916ed236010c8f8de19a3be2ee83169b65823226526f55c17db\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/11/79/e70e668095c0bb1f94718af672ef2d35ee7a023fee56ef54d9\n",
            "Successfully built fairscale antlr4-python3-runtime sacremoses contextvars\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, platformdirs, distlib, virtualenv, psutil, grpcio, dask, boto3, tensorboardX, ray, pyDeprecate, py4j, Pillow, distributed, autogluon.common, typish, torchmetrics, tokenizers, pyflakes, pycodestyle, portalocker, mccabe, immutables, hyperopt, huggingface-hub, colorama, autogluon.core, antlr4-python3-runtime, yacs, transformers, timm, sentencepiece, scikit-image, sacremoses, sacrebleu, pytorch-metric-learning, pytorch-lightning, pmdarima, omegaconf, nptyping, nlpaug, gluonts, flake8, fairscale, deprecated, contextvars, autogluon.features, autocfg, xgboost, tbats, sktime, lightgbm, gluoncv, catboost, autogluon.timeseries, autogluon.tabular, autogluon.multimodal, autogluon-contrib-nlp, autogluon.vision, autogluon.text, autogluon\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.49.1\n",
            "    Uninstalling grpcio-1.49.1:\n",
            "      Successfully uninstalled grpcio-1.49.1\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2022.2.0\n",
            "    Uninstalling dask-2022.2.0:\n",
            "      Successfully uninstalled dask-2022.2.0\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 2022.2.0\n",
            "    Uninstalling distributed-2022.2.0:\n",
            "      Successfully uninstalled distributed-2022.2.0\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed Pillow-9.0.1 antlr4-python3-runtime-4.8 autocfg-0.0.8 autogluon-0.5.2 autogluon-contrib-nlp-0.0.1b20220208 autogluon.common-0.5.2 autogluon.core-0.5.2 autogluon.features-0.5.2 autogluon.multimodal-0.5.2 autogluon.tabular-0.5.2 autogluon.text-0.5.2 autogluon.timeseries-0.5.2 autogluon.vision-0.5.2 boto3-1.25.0 botocore-1.28.0 catboost-1.0.6 colorama-0.4.6 contextvars-2.4 dask-2021.11.2 deprecated-1.2.13 distlib-0.3.6 distributed-2021.11.2 fairscale-0.4.6 flake8-3.9.2 gluoncv-0.10.5.post0 gluonts-0.9.9 grpcio-1.43.0 huggingface-hub-0.10.1 hyperopt-0.2.7 immutables-0.19 jmespath-1.0.1 lightgbm-3.3.3 mccabe-0.6.1 nlpaug-1.1.10 nptyping-1.4.4 omegaconf-2.1.2 platformdirs-2.5.2 pmdarima-1.8.5 portalocker-2.6.0 psutil-5.8.0 py4j-0.10.9.7 pyDeprecate-0.3.2 pycodestyle-2.7.0 pyflakes-2.3.1 pytorch-lightning-1.6.5 pytorch-metric-learning-1.3.2 ray-1.13.0 s3transfer-0.6.0 sacrebleu-2.3.1 sacremoses-0.0.53 scikit-image-0.19.3 sentencepiece-0.1.95 sktime-0.11.4 tbats-1.1.1 tensorboardX-2.5.1 timm-0.5.4 tokenizers-0.12.1 torchmetrics-0.7.3 transformers-4.20.1 typish-1.9.3 urllib3-1.25.11 virtualenv-20.16.5 xgboost-1.4.2 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "contextvars",
                  "psutil",
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install autogluon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I8NGNVfXEIS9"
      },
      "outputs": [],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Performance of App Recognition on NordVPN Dataset (CV1)**"
      ],
      "metadata": {
        "id": "Vl8S3LkoGPp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(f'NordVPN_CV1_train.csv', )\n",
        "df_test = pd.read_csv(f'NordVPN_CV1_test.csv', )\n",
        "\n",
        "#df_train = pd.read_csv(f'../dataset/NordVPN/NordVPN_CV1_train.csv', )\n",
        "#df_test = pd.read_csv(f'../dataset/NordVPN/NordVPN_CV1_test.csv', )\n",
        "\n",
        "title_small = []\n",
        "\n",
        "### Feature Vector Size : 1,670 (average size of packet length sequnece in train set.)\n",
        "for i in range(1670):\n",
        "  title_small.append(str(i))\n",
        "\n",
        "df_train[title_small] = (df_train['Packet_Length_Sequence'].str.split(' ', expand=True)).drop(list(range(1670, 10000)), axis='columns')\n",
        "df_test[title_small] = (df_test['Packet_Length_Sequence'].str.split(' ', expand=True)).drop(list(range(1670, 10000)), axis='columns')\n",
        "\n",
        "\n",
        "df_train = df_train.drop(labels=['Packet_Length_Sequence'], axis='columns')   \n",
        "df_test = df_test.drop(labels=['Packet_Length_Sequence'], axis='columns')   \n",
        "\n",
        "save_path = 'ag_models/'\n",
        "\n",
        "predictor = TabularPredictor(label=\"Label\", problem_type='multiclass', eval_metric='f1_weighted', path=save_path).fit(df_train, time_limit=600)\n",
        "\n",
        "results = predictor.fit_summary(show_plot=True)\n",
        "\n",
        "y_test = df_test['Label']    \n",
        "df_test = df_test.drop(labels=['Label'], axis=1)   \n",
        "y_pred = predictor.predict(df_test)\n",
        "y_pred_prob = predictor.predict_proba(df_test)\n",
        "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
        "labels_100 = [int(i) for i in range(100)]\n",
        "print(classification_report(y_test, y_pred, labels=labels_100, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYS6QtD7e5XC",
        "outputId": "d1e412da-d67b-49e7-b90d-a499bd6b9f98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 1670\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    12169.49 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 100.29 MB (0.8% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 1670 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 1670 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t11.4s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t1670 features in original data used to generate 1670 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 2.64 MB (0.0% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 11.97s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 588.03s of the 588.0s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 587.96s of the 587.94s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 587.9s of the 587.88s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.562\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t57.4s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t1.57s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 528.44s of the 528.42s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's multi_logloss: 2.81629\tvalid_set's f1_weighted: 0.315429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3274\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t477.26s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t2.72s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBM ... Training model for up to 44.22s of the 44.18s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 87. Best iteration is:\n",
            "\t[87]\tvalid_set's multi_logloss: 3.67173\tvalid_set's f1_weighted: 0.19569\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.1957\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t45.83s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t1.51s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -3.76s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5876\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.68s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 604.75s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.587635       4.287366  535.336953                0.001533           0.675320            2       True          4\n",
            "1      NeuralNetFastAI   0.562048       1.567548   57.400512                1.567548          57.400512            1       True          1\n",
            "2           LightGBMXT   0.327357       2.718284  477.261121                2.718284         477.261121            1       True          2\n",
            "3             LightGBM   0.195690       1.505276   45.825422                1.505276          45.825422            1       True          3\n",
            "Number of models trained: 4\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 1670 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6260196887118942\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6260196887118942,\n",
            "    \"accuracy\": 0.642,\n",
            "    \"balanced_accuracy\": 0.6420000000000001,\n",
            "    \"mcc\": 0.639275571731759\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7000    0.7000    0.7000        10\n",
            "           1     0.6667    0.8000    0.7273        10\n",
            "           2     0.6250    1.0000    0.7692        10\n",
            "           3     0.4615    0.6000    0.5217        10\n",
            "           4     0.6154    0.8000    0.6957        10\n",
            "           5     0.8571    0.6000    0.7059        10\n",
            "           6     1.0000    0.9000    0.9474        10\n",
            "           7     0.8333    0.5000    0.6250        10\n",
            "           8     0.7500    0.6000    0.6667        10\n",
            "           9     1.0000    0.9000    0.9474        10\n",
            "          10     0.7778    0.7000    0.7368        10\n",
            "          11     0.6429    0.9000    0.7500        10\n",
            "          12     0.6000    0.3000    0.4000        10\n",
            "          13     1.0000    0.8000    0.8889        10\n",
            "          14     0.8333    0.5000    0.6250        10\n",
            "          15     0.7500    0.6000    0.6667        10\n",
            "          16     0.7143    1.0000    0.8333        10\n",
            "          17     0.4615    0.6000    0.5217        10\n",
            "          18     0.0000    0.0000    0.0000        10\n",
            "          19     0.8182    0.9000    0.8571        10\n",
            "          20     0.8333    1.0000    0.9091        10\n",
            "          21     0.6154    0.8000    0.6957        10\n",
            "          22     0.6429    0.9000    0.7500        10\n",
            "          23     0.4444    0.4000    0.4211        10\n",
            "          24     0.8333    0.5000    0.6250        10\n",
            "          25     0.8000    0.8000    0.8000        10\n",
            "          26     1.0000    0.1000    0.1818        10\n",
            "          27     0.2632    0.5000    0.3448        10\n",
            "          28     0.4706    0.8000    0.5926        10\n",
            "          29     1.0000    0.1000    0.1818        10\n",
            "          30     1.0000    0.6000    0.7500        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     1.0000    0.4000    0.5714        10\n",
            "          33     0.5000    0.6000    0.5455        10\n",
            "          34     0.6000    0.9000    0.7200        10\n",
            "          35     1.0000    0.3000    0.4615        10\n",
            "          36     0.6250    1.0000    0.7692        10\n",
            "          37     0.5714    0.4000    0.4706        10\n",
            "          38     0.4000    0.2000    0.2667        10\n",
            "          39     0.5714    0.4000    0.4706        10\n",
            "          40     0.7143    0.5000    0.5882        10\n",
            "          41     1.0000    0.2000    0.3333        10\n",
            "          42     0.4167    0.5000    0.4545        10\n",
            "          43     1.0000    0.1000    0.1818        10\n",
            "          44     0.8333    0.5000    0.6250        10\n",
            "          45     0.4000    0.6000    0.4800        10\n",
            "          46     0.6154    0.8000    0.6957        10\n",
            "          47     0.9000    0.9000    0.9000        10\n",
            "          48     0.8571    0.6000    0.7059        10\n",
            "          49     1.0000    0.8000    0.8889        10\n",
            "          50     0.6000    0.9000    0.7200        10\n",
            "          51     0.4762    1.0000    0.6452        10\n",
            "          52     0.7778    0.7000    0.7368        10\n",
            "          53     0.7500    0.6000    0.6667        10\n",
            "          54     0.7778    0.7000    0.7368        10\n",
            "          55     0.8182    0.9000    0.8571        10\n",
            "          56     0.5455    0.6000    0.5714        10\n",
            "          57     0.6923    0.9000    0.7826        10\n",
            "          58     1.0000    0.2000    0.3333        10\n",
            "          59     0.5000    0.3000    0.3750        10\n",
            "          60     0.6250    0.5000    0.5556        10\n",
            "          61     0.6667    1.0000    0.8000        10\n",
            "          62     0.5000    0.5000    0.5000        10\n",
            "          63     0.6364    0.7000    0.6667        10\n",
            "          64     0.3750    0.3000    0.3333        10\n",
            "          65     0.6364    0.7000    0.6667        10\n",
            "          66     0.6667    0.6000    0.6316        10\n",
            "          67     0.9000    0.9000    0.9000        10\n",
            "          68     0.5000    0.6000    0.5455        10\n",
            "          69     1.0000    0.4000    0.5714        10\n",
            "          70     0.7500    0.6000    0.6667        10\n",
            "          71     0.7500    0.9000    0.8182        10\n",
            "          72     0.8000    0.8000    0.8000        10\n",
            "          73     0.6000    0.9000    0.7200        10\n",
            "          74     0.7143    1.0000    0.8333        10\n",
            "          75     0.2000    0.4000    0.2667        10\n",
            "          76     0.7500    0.3000    0.4286        10\n",
            "          77     0.6154    0.8000    0.6957        10\n",
            "          78     0.7143    0.5000    0.5882        10\n",
            "          79     0.4000    0.8000    0.5333        10\n",
            "          80     0.2564    1.0000    0.4082        10\n",
            "          81     0.3333    0.3000    0.3158        10\n",
            "          82     0.6667    0.4000    0.5000        10\n",
            "          83     1.0000    0.7000    0.8235        10\n",
            "          84     0.8333    1.0000    0.9091        10\n",
            "          85     0.6250    0.5000    0.5556        10\n",
            "          86     0.2500    0.1000    0.1429        10\n",
            "          87     0.8182    0.9000    0.8571        10\n",
            "          88     0.4286    0.3000    0.3529        10\n",
            "          89     0.8000    0.4000    0.5333        10\n",
            "          90     0.8182    0.9000    0.8571        10\n",
            "          91     0.5714    0.8000    0.6667        10\n",
            "          92     0.8182    0.9000    0.8571        10\n",
            "          93     1.0000    0.9000    0.9474        10\n",
            "          94     0.5294    0.9000    0.6667        10\n",
            "          95     0.6667    0.6000    0.6316        10\n",
            "          96     0.6667    0.2000    0.3077        10\n",
            "          97     0.8333    1.0000    0.9091        10\n",
            "          98     0.7500    0.6000    0.6667        10\n",
            "          99     0.7143    1.0000    0.8333        10\n",
            "\n",
            "    accuracy                         0.6420      1000\n",
            "   macro avg     0.6913    0.6420    0.6260      1000\n",
            "weighted avg     0.6913    0.6420    0.6260      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Vector Size Evaluation (Size of Packet Length Sequence) - from 500 to 10000**"
      ],
      "metadata": {
        "id": "AZ8fu1H-GAil"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYX0b4biEOmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f641029-f83c-4e9a-f01b-d5798347c9e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 500\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    11825.58 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 30.33 MB (0.3% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t3.2s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t500 features in original data used to generate 500 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 0.81 MB (0.0% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 3.42s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 596.58s of the 596.57s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 596.57s of the 596.56s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 596.56s of the 596.55s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.4362\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t20.05s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.28s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 576.03s of the 576.02s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.2467\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t122.52s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBM ... Training model for up to 451.34s of the 451.34s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.2321\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t125.57s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.49s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: RandomForestGini ... Training model for up to 323.83s of the 323.83s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.2077\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t5.97s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.45s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: RandomForestEntr ... Training model for up to 317.11s of the 317.1s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.1556\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t8.88s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.43s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: CatBoost ... Training model for up to 307.46s of the 307.45s of remaining time.\n",
            "INFO:autogluon.tabular.models.catboost.callbacks:\tRan out of time, early stopping on iteration 1.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0141\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t1059.79s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.65s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -753.18s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.4439\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t1.13s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 1354.59s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.443929       2.308801  1218.345001                0.001588           1.132007            2       True          7\n",
            "1      NeuralNetFastAI   0.436167       0.280591    20.054441                0.280591          20.054441            1       True          1\n",
            "2           LightGBMXT   0.246714       0.504224   122.520819                0.504224         122.520819            1       True          2\n",
            "3             LightGBM   0.232127       0.494443   125.570919                0.494443         125.570919            1       True          3\n",
            "4     RandomForestGini   0.207738       0.448609     5.965704                0.448609           5.965704            1       True          4\n",
            "5     RandomForestEntr   0.155571       0.425511     8.879951                0.425511           8.879951            1       True          5\n",
            "6             CatBoost   0.014104       0.648279  1059.792080                0.648279        1059.792080            1       True          6\n",
            "Number of models trained: 7\n",
            "Types of models trained:\n",
            "{'WeightedEnsembleModel', 'NNFastAiTabularModel', 'RFModel', 'LGBModel', 'CatBoostModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 500 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.4608481522182845\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.4608481522182845,\n",
            "    \"accuracy\": 0.484,\n",
            "    \"balanced_accuracy\": 0.48399999999999993,\n",
            "    \"mcc\": 0.4795096238118434\n",
            "}\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0833    0.1000    0.0909        10\n",
            "           1     0.3810    0.8000    0.5161        10\n",
            "           2     0.8000    0.8000    0.8000        10\n",
            "           3     0.7143    0.5000    0.5882        10\n",
            "           4     0.4000    0.2000    0.2667        10\n",
            "           5     0.2000    0.5000    0.2857        10\n",
            "           6     0.5294    0.9000    0.6667        10\n",
            "           7     0.2857    0.2000    0.2353        10\n",
            "           8     0.5333    0.8000    0.6400        10\n",
            "           9     0.7000    0.7000    0.7000        10\n",
            "          10     0.8889    0.8000    0.8421        10\n",
            "          11     1.0000    0.4000    0.5714        10\n",
            "          12     0.4545    0.5000    0.4762        10\n",
            "          13     0.6667    0.4000    0.5000        10\n",
            "          14     0.5000    0.1000    0.1667        10\n",
            "          15     0.3333    0.1000    0.1538        10\n",
            "          16     0.4375    0.7000    0.5385        10\n",
            "          17     0.3000    0.3000    0.3000        10\n",
            "          18     0.1111    0.1000    0.1053        10\n",
            "          19     0.5000    0.6000    0.5455        10\n",
            "          20     0.5714    0.8000    0.6667        10\n",
            "          21     0.5000    0.5000    0.5000        10\n",
            "          22     0.7143    0.5000    0.5882        10\n",
            "          23     0.4000    0.6000    0.4800        10\n",
            "          24     0.1818    0.2000    0.1905        10\n",
            "          25     0.4000    0.4000    0.4000        10\n",
            "          26     0.0000    0.0000    0.0000        10\n",
            "          27     0.5000    0.3000    0.3750        10\n",
            "          28     0.1250    0.1000    0.1111        10\n",
            "          29     0.0000    0.0000    0.0000        10\n",
            "          30     0.5000    0.9000    0.6429        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     0.2500    0.4000    0.3077        10\n",
            "          33     0.5000    0.3000    0.3750        10\n",
            "          34     0.4706    0.8000    0.5926        10\n",
            "          35     0.2727    0.3000    0.2857        10\n",
            "          36     0.7778    0.7000    0.7368        10\n",
            "          37     0.0000    0.0000    0.0000        10\n",
            "          38     0.2000    0.1000    0.1333        10\n",
            "          39     0.6667    0.4000    0.5000        10\n",
            "          40     0.4545    0.5000    0.4762        10\n",
            "          41     0.7500    0.3000    0.4286        10\n",
            "          42     0.0000    0.0000    0.0000        10\n",
            "          43     0.0000    0.0000    0.0000        10\n",
            "          44     0.2857    0.2000    0.2353        10\n",
            "          45     0.3333    0.1000    0.1538        10\n",
            "          46     0.5000    0.9000    0.6429        10\n",
            "          47     0.9091    1.0000    0.9524        10\n",
            "          48     0.5000    0.2000    0.2857        10\n",
            "          49     0.8750    0.7000    0.7778        10\n",
            "          50     0.7692    1.0000    0.8696        10\n",
            "          51     0.6250    1.0000    0.7692        10\n",
            "          52     0.7000    0.7000    0.7000        10\n",
            "          53     0.8000    0.8000    0.8000        10\n",
            "          54     0.4444    0.4000    0.4211        10\n",
            "          55     0.4000    0.6000    0.4800        10\n",
            "          56     0.0000    0.0000    0.0000        10\n",
            "          57     0.4545    0.5000    0.4762        10\n",
            "          58     0.3125    0.5000    0.3846        10\n",
            "          59     0.2500    0.3000    0.2727        10\n",
            "          60     0.2381    0.5000    0.3226        10\n",
            "          61     0.7500    0.3000    0.4286        10\n",
            "          62     0.6000    0.3000    0.4000        10\n",
            "          63     0.2857    0.4000    0.3333        10\n",
            "          64     1.0000    0.4000    0.5714        10\n",
            "          65     0.7273    0.8000    0.7619        10\n",
            "          66     0.5333    0.8000    0.6400        10\n",
            "          67     1.0000    0.9000    0.9474        10\n",
            "          68     0.7778    0.7000    0.7368        10\n",
            "          69     0.0000    0.0000    0.0000        10\n",
            "          70     0.4167    1.0000    0.5882        10\n",
            "          71     0.3333    0.5000    0.4000        10\n",
            "          72     0.6667    0.4000    0.5000        10\n",
            "          73     0.7500    0.9000    0.8182        10\n",
            "          74     0.5000    0.1000    0.1667        10\n",
            "          75     0.2857    0.2000    0.2353        10\n",
            "          76     0.5000    0.2000    0.2857        10\n",
            "          77     0.4545    0.5000    0.4762        10\n",
            "          78     0.6000    0.3000    0.4000        10\n",
            "          79     1.0000    0.4000    0.5714        10\n",
            "          80     0.2381    0.5000    0.3226        10\n",
            "          81     0.2500    0.1000    0.1429        10\n",
            "          82     0.2222    0.2000    0.2105        10\n",
            "          83     1.0000    0.7000    0.8235        10\n",
            "          84     0.8182    0.9000    0.8571        10\n",
            "          85     1.0000    0.3000    0.4615        10\n",
            "          86     0.2500    0.4000    0.3077        10\n",
            "          87     0.7500    0.9000    0.8182        10\n",
            "          88     0.7143    0.5000    0.5882        10\n",
            "          89     0.3000    0.6000    0.4000        10\n",
            "          90     0.5333    0.8000    0.6400        10\n",
            "          91     0.4118    0.7000    0.5185        10\n",
            "          92     0.6429    0.9000    0.7500        10\n",
            "          93     0.4286    0.3000    0.3529        10\n",
            "          94     0.2105    0.4000    0.2759        10\n",
            "          95     0.6667    0.6000    0.6316        10\n",
            "          96     0.5714    0.4000    0.4706        10\n",
            "          97     0.6154    0.8000    0.6957        10\n",
            "          98     0.2857    0.4000    0.3333        10\n",
            "          99     0.9091    1.0000    0.9524        10\n",
            "\n",
            "    accuracy                         0.4840      1000\n",
            "   macro avg     0.4945    0.4840    0.4608      1000\n",
            "weighted avg     0.4945    0.4840    0.4608      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10786.4 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 60.41 MB (0.6% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 1000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 1000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t6.4s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t1000 features in original data used to generate 1000 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 1.6 MB (0.0% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 6.79s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 593.21s of the 593.2s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 593.18s of the 593.17s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 593.15s of the 593.14s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5049\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t36.16s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.7s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 555.98s of the 555.97s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.306\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t246.31s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.83s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBM ... Training model for up to 307.12s of the 307.11s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 995. Best iteration is:\n",
            "\t[962]\tvalid_set's multi_logloss: 3.09427\tvalid_set's f1_weighted: 0.269817\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.2698\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t311.59s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t1.15s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -8.98s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5238\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.65s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 609.79s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.523810       1.529940  283.117537                0.001515           0.650788            2       True          4\n",
            "1      NeuralNetFastAI   0.504929       0.699996   36.161512                0.699996          36.161512            1       True          1\n",
            "2           LightGBMXT   0.305952       0.828429  246.305237                0.828429         246.305237            1       True          2\n",
            "3             LightGBM   0.269817       1.151347  311.594951                1.151347         311.594951            1       True          3\n",
            "Number of models trained: 4\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 1000 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.586518613485764\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.586518613485764,\n",
            "    \"accuracy\": 0.603,\n",
            "    \"balanced_accuracy\": 0.603,\n",
            "    \"mcc\": 0.5996480559234143\n",
            "}\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3077    0.4000    0.3478        10\n",
            "           1     0.5000    0.8000    0.6154        10\n",
            "           2     1.0000    0.7000    0.8235        10\n",
            "           3     0.4000    0.4000    0.4000        10\n",
            "           4     0.6000    0.9000    0.7200        10\n",
            "           5     1.0000    0.3000    0.4615        10\n",
            "           6     0.5294    0.9000    0.6667        10\n",
            "           7     0.2500    0.1000    0.1429        10\n",
            "           8     0.7273    0.8000    0.7619        10\n",
            "           9     1.0000    0.9000    0.9474        10\n",
            "          10     1.0000    0.6000    0.7500        10\n",
            "          11     0.4211    0.8000    0.5517        10\n",
            "          12     0.5000    0.5000    0.5000        10\n",
            "          13     0.7143    1.0000    0.8333        10\n",
            "          14     0.0000    0.0000    0.0000        10\n",
            "          15     0.2000    0.1000    0.1333        10\n",
            "          16     0.4444    0.8000    0.5714        10\n",
            "          17     0.5455    0.6000    0.5714        10\n",
            "          18     1.0000    0.1000    0.1818        10\n",
            "          19     0.5625    0.9000    0.6923        10\n",
            "          20     0.5625    0.9000    0.6923        10\n",
            "          21     0.5385    0.7000    0.6087        10\n",
            "          22     0.8333    0.5000    0.6250        10\n",
            "          23     0.7500    0.6000    0.6667        10\n",
            "          24     0.1429    0.1000    0.1176        10\n",
            "          25     0.8750    0.7000    0.7778        10\n",
            "          26     0.2857    0.4000    0.3333        10\n",
            "          27     0.5714    0.8000    0.6667        10\n",
            "          28     0.3333    0.5000    0.4000        10\n",
            "          29     0.0909    0.2000    0.1250        10\n",
            "          30     0.6923    0.9000    0.7826        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     0.8750    0.7000    0.7778        10\n",
            "          33     0.8333    0.5000    0.6250        10\n",
            "          34     0.6923    0.9000    0.7826        10\n",
            "          35     0.6250    0.5000    0.5556        10\n",
            "          36     0.8182    0.9000    0.8571        10\n",
            "          37     0.6667    0.4000    0.5000        10\n",
            "          38     0.3077    0.4000    0.3478        10\n",
            "          39     0.8333    0.5000    0.6250        10\n",
            "          40     0.8333    0.5000    0.6250        10\n",
            "          41     0.6000    0.3000    0.4000        10\n",
            "          42     0.6667    0.4000    0.5000        10\n",
            "          43     0.6000    0.3000    0.4000        10\n",
            "          44     0.5455    0.6000    0.5714        10\n",
            "          45     0.5000    0.6000    0.5455        10\n",
            "          46     0.6000    0.9000    0.7200        10\n",
            "          47     0.7692    1.0000    0.8696        10\n",
            "          48     0.6667    0.4000    0.5000        10\n",
            "          49     0.9091    1.0000    0.9524        10\n",
            "          50     0.9000    0.9000    0.9000        10\n",
            "          51     0.5000    1.0000    0.6667        10\n",
            "          52     0.8889    0.8000    0.8421        10\n",
            "          53     1.0000    0.3000    0.4615        10\n",
            "          54     0.5385    0.7000    0.6087        10\n",
            "          55     0.6429    0.9000    0.7500        10\n",
            "          56     0.3636    0.8000    0.5000        10\n",
            "          57     0.5455    0.6000    0.5714        10\n",
            "          58     0.7143    0.5000    0.5882        10\n",
            "          59     0.5000    0.2000    0.2857        10\n",
            "          60     0.3333    0.4000    0.3636        10\n",
            "          61     1.0000    0.7000    0.8235        10\n",
            "          62     0.7143    0.5000    0.5882        10\n",
            "          63     0.6364    0.7000    0.6667        10\n",
            "          64     1.0000    0.4000    0.5714        10\n",
            "          65     0.6667    1.0000    0.8000        10\n",
            "          66     0.5000    0.9000    0.6429        10\n",
            "          67     1.0000    0.9000    0.9474        10\n",
            "          68     0.5000    0.5000    0.5000        10\n",
            "          69     0.5714    0.4000    0.4706        10\n",
            "          70     0.5000    0.7000    0.5833        10\n",
            "          71     0.5833    0.7000    0.6364        10\n",
            "          72     0.6667    0.8000    0.7273        10\n",
            "          73     0.8182    0.9000    0.8571        10\n",
            "          74     0.9091    1.0000    0.9524        10\n",
            "          75     0.3333    0.2000    0.2500        10\n",
            "          76     0.3333    0.2000    0.2500        10\n",
            "          77     1.0000    0.7000    0.8235        10\n",
            "          78     0.4167    0.5000    0.4545        10\n",
            "          79     0.4444    0.8000    0.5714        10\n",
            "          80     0.3636    0.4000    0.3810        10\n",
            "          81     0.3000    0.3000    0.3000        10\n",
            "          82     0.5000    0.3000    0.3750        10\n",
            "          83     1.0000    0.7000    0.8235        10\n",
            "          84     0.7143    1.0000    0.8333        10\n",
            "          85     0.6000    0.3000    0.4000        10\n",
            "          86     0.3333    0.1000    0.1538        10\n",
            "          87     0.8889    0.8000    0.8421        10\n",
            "          88     0.6667    0.6000    0.6316        10\n",
            "          89     0.7500    0.3000    0.4286        10\n",
            "          90     0.9000    0.9000    0.9000        10\n",
            "          91     0.4167    0.5000    0.4545        10\n",
            "          92     0.9000    0.9000    0.9000        10\n",
            "          93     0.6667    0.6000    0.6316        10\n",
            "          94     0.7273    0.8000    0.7619        10\n",
            "          95     0.8333    0.5000    0.6250        10\n",
            "          96     0.0000    0.0000    0.0000        10\n",
            "          97     0.5556    1.0000    0.7143        10\n",
            "          98     0.7500    0.3000    0.4286        10\n",
            "          99     0.8889    0.8000    0.8421        10\n",
            "\n",
            "    accuracy                         0.6030      1000\n",
            "   macro avg     0.6300    0.6030    0.5865      1000\n",
            "weighted avg     0.6300    0.6030    0.5865      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 1500\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10801.58 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 90.2 MB (0.8% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 1500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 1500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t9.9s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t1500 features in original data used to generate 1500 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 2.38 MB (0.0% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 10.39s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 589.61s of the 589.59s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 589.55s of the 589.54s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 589.5s of the 589.49s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5635\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t52.53s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t1.3s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 535.23s of the 535.21s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3187\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t290.48s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t1.37s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBM ... Training model for up to 242.1s of the 242.08s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 552. Best iteration is:\n",
            "\t[283]\tvalid_set's multi_logloss: 3.20715\tvalid_set's f1_weighted: 0.268905\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.2689\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t244.01s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t1.35s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -4.47s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5917\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.63s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 605.45s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.591714       4.021913  587.651479                0.003020           0.631482            2       True          4\n",
            "1      NeuralNetFastAI   0.563540       1.302665   52.527291                1.302665          52.527291            1       True          1\n",
            "2           LightGBMXT   0.318690       1.366014  290.479345                1.366014         290.479345            1       True          2\n",
            "3             LightGBM   0.268905       1.350213  244.013361                1.350213         244.013361            1       True          3\n",
            "Number of models trained: 4\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 1500 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6286440184165266\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6286440184165266,\n",
            "    \"accuracy\": 0.644,\n",
            "    \"balanced_accuracy\": 0.644,\n",
            "    \"mcc\": 0.6410609788597038\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6667    0.6000    0.6316        10\n",
            "           1     0.5833    0.7000    0.6364        10\n",
            "           2     0.6250    1.0000    0.7692        10\n",
            "           3     0.5833    0.7000    0.6364        10\n",
            "           4     0.5000    0.5000    0.5000        10\n",
            "           5     0.3333    0.6000    0.4286        10\n",
            "           6     0.8182    0.9000    0.8571        10\n",
            "           7     0.6667    0.6000    0.6316        10\n",
            "           8     1.0000    0.5000    0.6667        10\n",
            "           9     0.9000    0.9000    0.9000        10\n",
            "          10     0.7000    0.7000    0.7000        10\n",
            "          11     0.6250    1.0000    0.7692        10\n",
            "          12     1.0000    0.2000    0.3333        10\n",
            "          13     0.7273    0.8000    0.7619        10\n",
            "          14     1.0000    0.2000    0.3333        10\n",
            "          15     0.8750    0.7000    0.7778        10\n",
            "          16     0.5263    1.0000    0.6897        10\n",
            "          17     0.1111    0.1000    0.1053        10\n",
            "          18     0.1667    0.1000    0.1250        10\n",
            "          19     0.6923    0.9000    0.7826        10\n",
            "          20     0.8000    0.8000    0.8000        10\n",
            "          21     0.6667    0.6000    0.6316        10\n",
            "          22     0.7778    0.7000    0.7368        10\n",
            "          23     0.4286    0.3000    0.3529        10\n",
            "          24     1.0000    0.4000    0.5714        10\n",
            "          25     0.8000    0.8000    0.8000        10\n",
            "          26     0.6667    0.2000    0.3077        10\n",
            "          27     0.6667    0.4000    0.5000        10\n",
            "          28     0.6250    0.5000    0.5556        10\n",
            "          29     0.2500    0.3000    0.2727        10\n",
            "          30     0.6000    0.9000    0.7200        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     1.0000    0.7000    0.8235        10\n",
            "          33     0.8333    0.5000    0.6250        10\n",
            "          34     0.9091    1.0000    0.9524        10\n",
            "          35     0.5455    0.6000    0.5714        10\n",
            "          36     0.5556    1.0000    0.7143        10\n",
            "          37     1.0000    0.1000    0.1818        10\n",
            "          38     0.5000    0.5000    0.5000        10\n",
            "          39     0.5000    0.4000    0.4444        10\n",
            "          40     0.6000    0.6000    0.6000        10\n",
            "          41     0.5714    0.4000    0.4706        10\n",
            "          42     0.2500    0.2000    0.2222        10\n",
            "          43     0.6000    0.3000    0.4000        10\n",
            "          44     0.5455    0.6000    0.5714        10\n",
            "          45     0.7500    0.6000    0.6667        10\n",
            "          46     0.5625    0.9000    0.6923        10\n",
            "          47     0.9091    1.0000    0.9524        10\n",
            "          48     0.6000    0.6000    0.6000        10\n",
            "          49     0.8333    1.0000    0.9091        10\n",
            "          50     0.6667    1.0000    0.8000        10\n",
            "          51     0.6250    1.0000    0.7692        10\n",
            "          52     0.8889    0.8000    0.8421        10\n",
            "          53     0.6000    0.9000    0.7200        10\n",
            "          54     0.5455    0.6000    0.5714        10\n",
            "          55     0.6667    0.8000    0.7273        10\n",
            "          56     0.6250    0.5000    0.5556        10\n",
            "          57     0.6667    0.6000    0.6316        10\n",
            "          58     0.5714    0.4000    0.4706        10\n",
            "          59     0.3000    0.3000    0.3000        10\n",
            "          60     0.7143    0.5000    0.5882        10\n",
            "          61     1.0000    0.4000    0.5714        10\n",
            "          62     1.0000    0.3000    0.4615        10\n",
            "          63     0.6154    0.8000    0.6957        10\n",
            "          64     1.0000    0.4000    0.5714        10\n",
            "          65     0.6000    0.9000    0.7200        10\n",
            "          66     0.4375    0.7000    0.5385        10\n",
            "          67     0.9000    0.9000    0.9000        10\n",
            "          68     0.6667    0.8000    0.7273        10\n",
            "          69     0.7273    0.8000    0.7619        10\n",
            "          70     0.7778    0.7000    0.7368        10\n",
            "          71     0.7778    0.7000    0.7368        10\n",
            "          72     0.3600    0.9000    0.5143        10\n",
            "          73     0.9000    0.9000    0.9000        10\n",
            "          74     0.7143    1.0000    0.8333        10\n",
            "          75     0.5000    0.2000    0.2857        10\n",
            "          76     0.5000    0.5000    0.5000        10\n",
            "          77     0.7000    0.7000    0.7000        10\n",
            "          78     0.7273    0.8000    0.7619        10\n",
            "          79     0.7500    0.9000    0.8182        10\n",
            "          80     0.3810    0.8000    0.5161        10\n",
            "          81     0.5000    0.3000    0.3750        10\n",
            "          82     1.0000    0.4000    0.5714        10\n",
            "          83     1.0000    0.8000    0.8889        10\n",
            "          84     0.9091    1.0000    0.9524        10\n",
            "          85     0.7500    0.3000    0.4286        10\n",
            "          86     0.5000    0.2000    0.2857        10\n",
            "          87     1.0000    0.9000    0.9474        10\n",
            "          88     0.4444    0.4000    0.4211        10\n",
            "          89     1.0000    0.5000    0.6667        10\n",
            "          90     0.5000    0.9000    0.6429        10\n",
            "          91     0.5000    0.7000    0.5833        10\n",
            "          92     0.9000    0.9000    0.9000        10\n",
            "          93     0.8333    0.5000    0.6250        10\n",
            "          94     0.6667    1.0000    0.8000        10\n",
            "          95     0.7500    0.6000    0.6667        10\n",
            "          96     1.0000    0.4000    0.5714        10\n",
            "          97     0.4348    1.0000    0.6061        10\n",
            "          98     0.7500    0.6000    0.6667        10\n",
            "          99     0.8333    1.0000    0.9091        10\n",
            "\n",
            "    accuracy                         0.6440      1000\n",
            "   macro avg     0.6862    0.6440    0.6286      1000\n",
            "weighted avg     0.6862    0.6440    0.6286      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 2000\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10802.84 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 119.81 MB (1.1% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 2000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 2000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t13.6s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t2000 features in original data used to generate 2000 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 3.14 MB (0.0% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 14.22s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 585.78s of the 585.75s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 585.7s of the 585.67s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 585.61s of the 585.58s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5724\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t64.27s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t2.1s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 518.64s of the 518.62s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3255\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t285.94s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t2.02s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBM ... Training model for up to 229.55s of the 229.52s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 451. Best iteration is:\n",
            "\t[424]\tvalid_set's multi_logloss: 3.02652\tvalid_set's f1_weighted: 0.306476\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3065\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t232.73s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t3.93s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -10.58s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5934\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.62s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 611.64s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.593381       8.057282  583.564485                0.001513           0.624462            2       True          4\n",
            "1      NeuralNetFastAI   0.572381       2.102535   64.270787                2.102535          64.270787            1       True          1\n",
            "2           LightGBMXT   0.325476       2.023136  285.942860                2.023136         285.942860            1       True          2\n",
            "3             LightGBM   0.306476       3.930098  232.726376                3.930098         232.726376            1       True          3\n",
            "Number of models trained: 4\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 2000 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6260353521211444\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6260353521211444,\n",
            "    \"accuracy\": 0.641,\n",
            "    \"balanced_accuracy\": 0.6409999999999999,\n",
            "    \"mcc\": 0.638066318529431\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5294    0.9000    0.6667        10\n",
            "           1     1.0000    0.8000    0.8889        10\n",
            "           2     0.5000    1.0000    0.6667        10\n",
            "           3     0.4667    0.7000    0.5600        10\n",
            "           4     0.8000    0.8000    0.8000        10\n",
            "           5     0.6250    0.5000    0.5556        10\n",
            "           6     0.6923    0.9000    0.7826        10\n",
            "           7     0.5714    0.4000    0.4706        10\n",
            "           8     0.7778    0.7000    0.7368        10\n",
            "           9     1.0000    0.9000    0.9474        10\n",
            "          10     0.7000    0.7000    0.7000        10\n",
            "          11     0.7143    1.0000    0.8333        10\n",
            "          12     1.0000    0.4000    0.5714        10\n",
            "          13     0.8571    0.6000    0.7059        10\n",
            "          14     0.5000    0.1000    0.1667        10\n",
            "          15     0.4615    0.6000    0.5217        10\n",
            "          16     0.7143    1.0000    0.8333        10\n",
            "          17     0.2222    0.2000    0.2105        10\n",
            "          18     0.0000    0.0000    0.0000        10\n",
            "          19     0.6154    0.8000    0.6957        10\n",
            "          20     0.7778    0.7000    0.7368        10\n",
            "          21     0.8000    0.8000    0.8000        10\n",
            "          22     0.6667    0.4000    0.5000        10\n",
            "          23     0.5000    0.4000    0.4444        10\n",
            "          24     0.5000    0.3000    0.3750        10\n",
            "          25     0.8333    0.5000    0.6250        10\n",
            "          26     0.6667    0.2000    0.3077        10\n",
            "          27     0.4615    0.6000    0.5217        10\n",
            "          28     1.0000    0.3000    0.4615        10\n",
            "          29     0.3333    0.1000    0.1538        10\n",
            "          30     0.5333    0.8000    0.6400        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     0.7500    0.3000    0.4286        10\n",
            "          33     0.5294    0.9000    0.6667        10\n",
            "          34     0.5882    1.0000    0.7407        10\n",
            "          35     0.3889    0.7000    0.5000        10\n",
            "          36     0.8333    1.0000    0.9091        10\n",
            "          37     1.0000    0.4000    0.5714        10\n",
            "          38     0.5556    0.5000    0.5263        10\n",
            "          39     0.5714    0.4000    0.4706        10\n",
            "          40     0.7273    0.8000    0.7619        10\n",
            "          41     0.6000    0.3000    0.4000        10\n",
            "          42     1.0000    0.2000    0.3333        10\n",
            "          43     1.0000    0.3000    0.4615        10\n",
            "          44     1.0000    0.6000    0.7500        10\n",
            "          45     0.8571    0.6000    0.7059        10\n",
            "          46     0.5625    0.9000    0.6923        10\n",
            "          47     0.8182    0.9000    0.8571        10\n",
            "          48     0.7778    0.7000    0.7368        10\n",
            "          49     0.8889    0.8000    0.8421        10\n",
            "          50     0.8333    1.0000    0.9091        10\n",
            "          51     0.5882    1.0000    0.7407        10\n",
            "          52     0.8889    0.8000    0.8421        10\n",
            "          53     0.7143    1.0000    0.8333        10\n",
            "          54     1.0000    0.4000    0.5714        10\n",
            "          55     0.4706    0.8000    0.5926        10\n",
            "          56     0.6364    0.7000    0.6667        10\n",
            "          57     0.5833    0.7000    0.6364        10\n",
            "          58     0.6667    0.2000    0.3077        10\n",
            "          59     0.8333    0.5000    0.6250        10\n",
            "          60     0.2308    0.3000    0.2609        10\n",
            "          61     0.5000    0.6000    0.5455        10\n",
            "          62     1.0000    0.5000    0.6667        10\n",
            "          63     0.7273    0.8000    0.7619        10\n",
            "          64     0.8333    0.5000    0.6250        10\n",
            "          65     0.6923    0.9000    0.7826        10\n",
            "          66     0.6364    0.7000    0.6667        10\n",
            "          67     0.9000    0.9000    0.9000        10\n",
            "          68     0.4667    0.7000    0.5600        10\n",
            "          69     0.8333    0.5000    0.6250        10\n",
            "          70     0.5714    0.4000    0.4706        10\n",
            "          71     0.8889    0.8000    0.8421        10\n",
            "          72     0.7500    0.6000    0.6667        10\n",
            "          73     1.0000    0.9000    0.9474        10\n",
            "          74     0.5000    1.0000    0.6667        10\n",
            "          75     1.0000    0.1000    0.1818        10\n",
            "          76     0.5000    0.7000    0.5833        10\n",
            "          77     0.8750    0.7000    0.7778        10\n",
            "          78     0.5000    0.8000    0.6154        10\n",
            "          79     0.5455    0.6000    0.5714        10\n",
            "          80     0.3600    0.9000    0.5143        10\n",
            "          81     0.3750    0.6000    0.4615        10\n",
            "          82     0.6000    0.6000    0.6000        10\n",
            "          83     0.8750    0.7000    0.7778        10\n",
            "          84     0.7143    1.0000    0.8333        10\n",
            "          85     0.6000    0.6000    0.6000        10\n",
            "          86     0.6000    0.3000    0.4000        10\n",
            "          87     0.9000    0.9000    0.9000        10\n",
            "          88     0.7500    0.3000    0.4286        10\n",
            "          89     0.5000    0.5000    0.5000        10\n",
            "          90     0.6429    0.9000    0.7500        10\n",
            "          91     0.3333    0.6000    0.4286        10\n",
            "          92     0.9000    0.9000    0.9000        10\n",
            "          93     1.0000    0.7000    0.8235        10\n",
            "          94     0.6000    0.6000    0.6000        10\n",
            "          95     0.4706    0.8000    0.5926        10\n",
            "          96     0.5714    0.4000    0.4706        10\n",
            "          97     0.7273    0.8000    0.7619        10\n",
            "          98     0.7778    0.7000    0.7368        10\n",
            "          99     0.9000    0.9000    0.9000        10\n",
            "\n",
            "    accuracy                         0.6410      1000\n",
            "   macro avg     0.6863    0.6410    0.6260      1000\n",
            "weighted avg     0.6863    0.6410    0.6260      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 2500\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10828.01 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 149.31 MB (1.4% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 2500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 2500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t17.3s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t2500 features in original data used to generate 2500 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 3.88 MB (0.0% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 18.14s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 581.86s of the 581.81s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 581.72s of the 581.68s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 581.59s of the 581.54s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6393\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t82.02s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t2.79s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 495.86s of the 495.81s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's multi_logloss: 3.34328\tvalid_set's f1_weighted: 0.323786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 1134. Best iteration is:\n",
            "\t[1055]\tvalid_set's multi_logloss: 3.35389\tvalid_set's f1_weighted: 0.328786\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3288\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t501.84s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t3.58s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -14.33s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6463\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.48s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 615.27s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.646333       6.372387  584.351736                0.001830           0.484924            2       True          3\n",
            "1      NeuralNetFastAI   0.639333       2.792379   82.022053                2.792379          82.022053            1       True          1\n",
            "2           LightGBMXT   0.328786       3.578179  501.844759                3.578179         501.844759            1       True          2\n",
            "Number of models trained: 3\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 2500 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6469146620735227\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6469146620735227,\n",
            "    \"accuracy\": 0.656,\n",
            "    \"balanced_accuracy\": 0.6559999999999999,\n",
            "    \"mcc\": 0.6532018978846311\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5714    0.8000    0.6667        10\n",
            "           1     0.5882    1.0000    0.7407        10\n",
            "           2     0.6667    1.0000    0.8000        10\n",
            "           3     0.5000    0.5000    0.5000        10\n",
            "           4     0.5714    0.8000    0.6667        10\n",
            "           5     0.7500    0.6000    0.6667        10\n",
            "           6     0.9000    0.9000    0.9000        10\n",
            "           7     0.5000    0.6000    0.5455        10\n",
            "           8     0.7778    0.7000    0.7368        10\n",
            "           9     1.0000    1.0000    1.0000        10\n",
            "          10     0.8750    0.7000    0.7778        10\n",
            "          11     0.7500    0.9000    0.8182        10\n",
            "          12     1.0000    0.2000    0.3333        10\n",
            "          13     0.7143    0.5000    0.5882        10\n",
            "          14     0.7500    0.3000    0.4286        10\n",
            "          15     0.3500    0.7000    0.4667        10\n",
            "          16     0.7692    1.0000    0.8696        10\n",
            "          17     0.3000    0.3000    0.3000        10\n",
            "          18     0.0000    0.0000    0.0000        10\n",
            "          19     0.5625    0.9000    0.6923        10\n",
            "          20     1.0000    0.8000    0.8889        10\n",
            "          21     0.6667    0.8000    0.7273        10\n",
            "          22     0.4286    0.6000    0.5000        10\n",
            "          23     0.4000    0.6000    0.4800        10\n",
            "          24     0.8000    0.4000    0.5333        10\n",
            "          25     1.0000    0.3000    0.4615        10\n",
            "          26     0.7500    0.3000    0.4286        10\n",
            "          27     0.2500    0.1000    0.1429        10\n",
            "          28     0.7143    0.5000    0.5882        10\n",
            "          29     0.2500    0.2000    0.2222        10\n",
            "          30     0.8182    0.9000    0.8571        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     0.8571    0.6000    0.7059        10\n",
            "          33     0.7500    0.6000    0.6667        10\n",
            "          34     0.6250    1.0000    0.7692        10\n",
            "          35     0.6250    0.5000    0.5556        10\n",
            "          36     0.8333    1.0000    0.9091        10\n",
            "          37     0.5556    0.5000    0.5263        10\n",
            "          38     0.2500    0.6000    0.3529        10\n",
            "          39     0.6154    0.8000    0.6957        10\n",
            "          40     1.0000    0.7000    0.8235        10\n",
            "          41     0.5000    0.3000    0.3750        10\n",
            "          42     0.7500    0.6000    0.6667        10\n",
            "          43     0.5556    0.5000    0.5263        10\n",
            "          44     1.0000    0.4000    0.5714        10\n",
            "          45     0.6667    0.6000    0.6316        10\n",
            "          46     0.6429    0.9000    0.7500        10\n",
            "          47     0.7692    1.0000    0.8696        10\n",
            "          48     0.8571    0.6000    0.7059        10\n",
            "          49     0.8182    0.9000    0.8571        10\n",
            "          50     0.9091    1.0000    0.9524        10\n",
            "          51     0.6250    1.0000    0.7692        10\n",
            "          52     0.7273    0.8000    0.7619        10\n",
            "          53     0.5000    0.6000    0.5455        10\n",
            "          54     1.0000    0.5000    0.6667        10\n",
            "          55     0.7273    0.8000    0.7619        10\n",
            "          56     1.0000    0.5000    0.6667        10\n",
            "          57     0.6667    0.6000    0.6316        10\n",
            "          58     0.5000    0.3000    0.3750        10\n",
            "          59     0.5000    0.4000    0.4444        10\n",
            "          60     0.6364    0.7000    0.6667        10\n",
            "          61     0.6154    0.8000    0.6957        10\n",
            "          62     0.7500    0.3000    0.4286        10\n",
            "          63     0.6364    0.7000    0.6667        10\n",
            "          64     0.8000    0.4000    0.5333        10\n",
            "          65     0.6000    0.9000    0.7200        10\n",
            "          66     0.3500    0.7000    0.4667        10\n",
            "          67     0.7500    0.9000    0.8182        10\n",
            "          68     0.7273    0.8000    0.7619        10\n",
            "          69     0.8000    0.4000    0.5333        10\n",
            "          70     0.6000    0.6000    0.6000        10\n",
            "          71     0.6364    0.7000    0.6667        10\n",
            "          72     1.0000    0.8000    0.8889        10\n",
            "          73     0.8333    1.0000    0.9091        10\n",
            "          74     0.7143    1.0000    0.8333        10\n",
            "          75     0.2500    0.2000    0.2222        10\n",
            "          76     0.5714    0.4000    0.4706        10\n",
            "          77     0.8889    0.8000    0.8421        10\n",
            "          78     0.7500    0.6000    0.6667        10\n",
            "          79     0.8333    0.5000    0.6250        10\n",
            "          80     0.2581    0.8000    0.3902        10\n",
            "          81     0.6250    0.5000    0.5556        10\n",
            "          82     0.7500    0.3000    0.4286        10\n",
            "          83     1.0000    0.8000    0.8889        10\n",
            "          84     0.7143    1.0000    0.8333        10\n",
            "          85     1.0000    0.6000    0.7500        10\n",
            "          86     0.5000    0.4000    0.4444        10\n",
            "          87     0.7500    0.9000    0.8182        10\n",
            "          88     0.8333    0.5000    0.6250        10\n",
            "          89     0.8000    0.8000    0.8000        10\n",
            "          90     0.7500    0.9000    0.8182        10\n",
            "          91     0.5714    0.8000    0.6667        10\n",
            "          92     0.7500    0.9000    0.8182        10\n",
            "          93     0.7500    0.6000    0.6667        10\n",
            "          94     0.7273    0.8000    0.7619        10\n",
            "          95     0.7500    0.6000    0.6667        10\n",
            "          96     1.0000    0.5000    0.6667        10\n",
            "          97     0.8571    0.6000    0.7059        10\n",
            "          98     0.6000    0.9000    0.7200        10\n",
            "          99     0.8889    0.8000    0.8421        10\n",
            "\n",
            "    accuracy                         0.6560      1000\n",
            "   macro avg     0.6942    0.6560    0.6469      1000\n",
            "weighted avg     0.6942    0.6560    0.6469      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 3000\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10844.63 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 178.72 MB (1.6% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 3000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 3000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t24.3s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t3000 features in original data used to generate 3000 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 4.65 MB (0.0% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 25.34s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 574.66s of the 574.6s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 574.47s of the 574.41s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 574.29s of the 574.23s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5699\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t94.03s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t3.96s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 475.46s of the 475.4s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.2995\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t359.84s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t4.0s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBM ... Training model for up to 109.73s of the 109.67s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 183. Best iteration is:\n",
            "\t[166]\tvalid_set's multi_logloss: 3.40332\tvalid_set's f1_weighted: 0.253833\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.2538\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t112.21s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t3.78s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -7.98s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5699\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.67s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 609.21s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0      NeuralNetFastAI   0.569905       3.961912   94.025621                3.961912          94.025621            1       True          1\n",
            "1  WeightedEnsemble_L2   0.569905       3.963510   94.699815                0.001598           0.674194            2       True          4\n",
            "2           LightGBMXT   0.299548       4.001411  359.840893                4.001411         359.840893            1       True          2\n",
            "3             LightGBM   0.253833       3.781138  112.208291                3.781138         112.208291            1       True          3\n",
            "Number of models trained: 4\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 3000 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6217463767660225\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6217463767660225,\n",
            "    \"accuracy\": 0.632,\n",
            "    \"balanced_accuracy\": 0.632,\n",
            "    \"mcc\": 0.6291579027721625\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5714    0.8000    0.6667        10\n",
            "           1     1.0000    0.9000    0.9474        10\n",
            "           2     0.6667    1.0000    0.8000        10\n",
            "           3     0.6154    0.8000    0.6957        10\n",
            "           4     0.5385    0.7000    0.6087        10\n",
            "           5     0.7143    0.5000    0.5882        10\n",
            "           6     0.6000    0.9000    0.7200        10\n",
            "           7     0.6000    0.3000    0.4000        10\n",
            "           8     1.0000    0.8000    0.8889        10\n",
            "           9     0.9091    1.0000    0.9524        10\n",
            "          10     0.7778    0.7000    0.7368        10\n",
            "          11     0.5000    1.0000    0.6667        10\n",
            "          12     0.6667    0.2000    0.3077        10\n",
            "          13     0.8571    0.6000    0.7059        10\n",
            "          14     0.7500    0.3000    0.4286        10\n",
            "          15     0.3125    0.5000    0.3846        10\n",
            "          16     0.6667    1.0000    0.8000        10\n",
            "          17     0.6667    0.2000    0.3077        10\n",
            "          18     0.4000    0.4000    0.4000        10\n",
            "          19     0.7500    0.9000    0.8182        10\n",
            "          20     0.6429    0.9000    0.7500        10\n",
            "          21     0.7273    0.8000    0.7619        10\n",
            "          22     0.5000    0.3000    0.3750        10\n",
            "          23     0.7500    0.6000    0.6667        10\n",
            "          24     0.6000    0.3000    0.4000        10\n",
            "          25     0.6250    0.5000    0.5556        10\n",
            "          26     0.5000    0.2000    0.2857        10\n",
            "          27     0.2500    0.4000    0.3077        10\n",
            "          28     0.7778    0.7000    0.7368        10\n",
            "          29     0.1778    0.8000    0.2909        10\n",
            "          30     0.5625    0.9000    0.6923        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     0.8333    0.5000    0.6250        10\n",
            "          33     1.0000    0.2000    0.3333        10\n",
            "          34     0.6250    1.0000    0.7692        10\n",
            "          35     0.8000    0.4000    0.5333        10\n",
            "          36     0.7692    1.0000    0.8696        10\n",
            "          37     0.7500    0.6000    0.6667        10\n",
            "          38     0.3333    0.3000    0.3158        10\n",
            "          39     0.6364    0.7000    0.6667        10\n",
            "          40     1.0000    0.7000    0.8235        10\n",
            "          41     0.5000    0.3000    0.3750        10\n",
            "          42     0.4000    0.8000    0.5333        10\n",
            "          43     0.6667    0.4000    0.5000        10\n",
            "          44     0.7143    0.5000    0.5882        10\n",
            "          45     0.7778    0.7000    0.7368        10\n",
            "          46     0.6923    0.9000    0.7826        10\n",
            "          47     0.8333    1.0000    0.9091        10\n",
            "          48     0.7500    0.6000    0.6667        10\n",
            "          49     0.7778    0.7000    0.7368        10\n",
            "          50     0.8182    0.9000    0.8571        10\n",
            "          51     0.5556    1.0000    0.7143        10\n",
            "          52     0.7273    0.8000    0.7619        10\n",
            "          53     0.6250    0.5000    0.5556        10\n",
            "          54     1.0000    0.6000    0.7500        10\n",
            "          55     0.7778    0.7000    0.7368        10\n",
            "          56     0.5000    0.6000    0.5455        10\n",
            "          57     0.6250    0.5000    0.5556        10\n",
            "          58     0.6000    0.3000    0.4000        10\n",
            "          59     0.6667    0.4000    0.5000        10\n",
            "          60     0.6667    0.4000    0.5000        10\n",
            "          61     0.8889    0.8000    0.8421        10\n",
            "          62     0.5000    0.4000    0.4444        10\n",
            "          63     0.8000    0.8000    0.8000        10\n",
            "          64     0.4167    0.5000    0.4545        10\n",
            "          65     0.8000    0.8000    0.8000        10\n",
            "          66     0.7273    0.8000    0.7619        10\n",
            "          67     0.9000    0.9000    0.9000        10\n",
            "          68     0.6250    0.5000    0.5556        10\n",
            "          69     1.0000    0.5000    0.6667        10\n",
            "          70     0.3077    0.4000    0.3478        10\n",
            "          71     0.5385    0.7000    0.6087        10\n",
            "          72     0.7500    0.3000    0.4286        10\n",
            "          73     0.8182    0.9000    0.8571        10\n",
            "          74     0.7143    1.0000    0.8333        10\n",
            "          75     0.3333    0.2000    0.2500        10\n",
            "          76     0.6667    0.6000    0.6316        10\n",
            "          77     0.4615    0.6000    0.5217        10\n",
            "          78     0.4615    0.6000    0.5217        10\n",
            "          79     0.5385    0.7000    0.6087        10\n",
            "          80     0.0000    0.0000    0.0000        10\n",
            "          81     0.3333    0.4000    0.3636        10\n",
            "          82     0.8000    0.4000    0.5333        10\n",
            "          83     1.0000    0.7000    0.8235        10\n",
            "          84     0.9091    1.0000    0.9524        10\n",
            "          85     0.8750    0.7000    0.7778        10\n",
            "          86     0.6667    0.2000    0.3077        10\n",
            "          87     0.7273    0.8000    0.7619        10\n",
            "          88     1.0000    0.5000    0.6667        10\n",
            "          89     0.7500    0.9000    0.8182        10\n",
            "          90     0.8000    0.8000    0.8000        10\n",
            "          91     0.3333    0.3000    0.3158        10\n",
            "          92     0.7500    0.9000    0.8182        10\n",
            "          93     0.5385    0.7000    0.6087        10\n",
            "          94     0.6250    0.5000    0.5556        10\n",
            "          95     0.7000    0.7000    0.7000        10\n",
            "          96     0.4000    0.2000    0.2667        10\n",
            "          97     0.5625    0.9000    0.6923        10\n",
            "          98     0.7500    0.9000    0.8182        10\n",
            "          99     0.8182    0.9000    0.8571        10\n",
            "\n",
            "    accuracy                         0.6320      1000\n",
            "   macro avg     0.6668    0.6320    0.6217      1000\n",
            "weighted avg     0.6668    0.6320    0.6217      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 3500\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10864.64 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 208.08 MB (1.9% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 3500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 3500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t25.5s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t3500 features in original data used to generate 3500 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 5.42 MB (0.0% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 26.68s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 573.32s of the 573.24s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 573.08s of the 572.99s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 572.84s of the 572.75s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6099\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t112.3s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t5.04s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 454.31s of the 454.23s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 927. Best iteration is:\n",
            "\t[916]\tvalid_set's multi_logloss: 3.36422\tvalid_set's f1_weighted: 0.319452\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3195\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t459.53s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t5.52s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -15.09s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6162\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.46s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 615.98s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.616214      10.564661  572.298708                0.001570           0.463898            2       True          3\n",
            "1      NeuralNetFastAI   0.609929       5.041043  112.301845                5.041043         112.301845            1       True          1\n",
            "2           LightGBMXT   0.319452       5.522049  459.532966                5.522049         459.532966            1       True          2\n",
            "Number of models trained: 3\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 3500 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6621552189361517\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6621552189361517,\n",
            "    \"accuracy\": 0.672,\n",
            "    \"balanced_accuracy\": 0.672,\n",
            "    \"mcc\": 0.6693497856365279\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6667    0.8000    0.7273        10\n",
            "           1     0.7500    0.9000    0.8182        10\n",
            "           2     0.9091    1.0000    0.9524        10\n",
            "           3     0.8571    0.6000    0.7059        10\n",
            "           4     0.6154    0.8000    0.6957        10\n",
            "           5     0.7143    0.5000    0.5882        10\n",
            "           6     0.8889    0.8000    0.8421        10\n",
            "           7     0.5455    0.6000    0.5714        10\n",
            "           8     0.5714    0.8000    0.6667        10\n",
            "           9     0.9000    0.9000    0.9000        10\n",
            "          10     0.8889    0.8000    0.8421        10\n",
            "          11     0.9091    1.0000    0.9524        10\n",
            "          12     1.0000    0.2000    0.3333        10\n",
            "          13     1.0000    0.6000    0.7500        10\n",
            "          14     1.0000    0.1000    0.1818        10\n",
            "          15     0.3636    0.4000    0.3810        10\n",
            "          16     0.7143    1.0000    0.8333        10\n",
            "          17     0.3500    0.7000    0.4667        10\n",
            "          18     0.0000    0.0000    0.0000        10\n",
            "          19     0.8000    0.8000    0.8000        10\n",
            "          20     0.6154    0.8000    0.6957        10\n",
            "          21     0.6364    0.7000    0.6667        10\n",
            "          22     0.8571    0.6000    0.7059        10\n",
            "          23     0.4545    0.5000    0.4762        10\n",
            "          24     0.8000    0.4000    0.5333        10\n",
            "          25     0.6923    0.9000    0.7826        10\n",
            "          26     0.3333    0.5000    0.4000        10\n",
            "          27     0.4375    0.7000    0.5385        10\n",
            "          28     0.7273    0.8000    0.7619        10\n",
            "          29     0.2222    0.2000    0.2105        10\n",
            "          30     0.6923    0.9000    0.7826        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     1.0000    0.8000    0.8889        10\n",
            "          33     1.0000    0.6000    0.7500        10\n",
            "          34     1.0000    0.9000    0.9474        10\n",
            "          35     0.2857    0.2000    0.2353        10\n",
            "          36     1.0000    1.0000    1.0000        10\n",
            "          37     0.3846    0.5000    0.4348        10\n",
            "          38     0.2500    0.1000    0.1429        10\n",
            "          39     0.4000    0.4000    0.4000        10\n",
            "          40     0.8000    0.8000    0.8000        10\n",
            "          41     0.4286    0.3000    0.3529        10\n",
            "          42     0.8571    0.6000    0.7059        10\n",
            "          43     0.6000    0.6000    0.6000        10\n",
            "          44     1.0000    0.1000    0.1818        10\n",
            "          45     0.8571    0.6000    0.7059        10\n",
            "          46     0.7273    0.8000    0.7619        10\n",
            "          47     0.8333    1.0000    0.9091        10\n",
            "          48     0.7778    0.7000    0.7368        10\n",
            "          49     0.9091    1.0000    0.9524        10\n",
            "          50     0.7500    0.9000    0.8182        10\n",
            "          51     0.5556    1.0000    0.7143        10\n",
            "          52     1.0000    1.0000    1.0000        10\n",
            "          53     0.7500    0.6000    0.6667        10\n",
            "          54     0.8333    0.5000    0.6250        10\n",
            "          55     0.7273    0.8000    0.7619        10\n",
            "          56     0.8571    0.6000    0.7059        10\n",
            "          57     0.5833    0.7000    0.6364        10\n",
            "          58     1.0000    0.4000    0.5714        10\n",
            "          59     0.5000    0.4000    0.4444        10\n",
            "          60     0.5000    0.6000    0.5455        10\n",
            "          61     1.0000    0.4000    0.5714        10\n",
            "          62     0.3846    0.5000    0.4348        10\n",
            "          63     0.7273    0.8000    0.7619        10\n",
            "          64     1.0000    0.5000    0.6667        10\n",
            "          65     0.3913    0.9000    0.5455        10\n",
            "          66     0.6667    1.0000    0.8000        10\n",
            "          67     1.0000    0.9000    0.9474        10\n",
            "          68     0.8571    0.6000    0.7059        10\n",
            "          69     1.0000    0.8000    0.8889        10\n",
            "          70     0.3750    0.9000    0.5294        10\n",
            "          71     0.6000    0.9000    0.7200        10\n",
            "          72     0.6250    0.5000    0.5556        10\n",
            "          73     1.0000    1.0000    1.0000        10\n",
            "          74     0.6667    1.0000    0.8000        10\n",
            "          75     0.4000    0.2000    0.2667        10\n",
            "          76     1.0000    0.4000    0.5714        10\n",
            "          77     0.6364    0.7000    0.6667        10\n",
            "          78     0.8000    0.4000    0.5333        10\n",
            "          79     1.0000    0.7000    0.8235        10\n",
            "          80     0.4615    0.6000    0.5217        10\n",
            "          81     0.4118    0.7000    0.5185        10\n",
            "          82     0.8000    0.4000    0.5333        10\n",
            "          83     1.0000    0.8000    0.8889        10\n",
            "          84     0.9091    1.0000    0.9524        10\n",
            "          85     0.6667    0.6000    0.6316        10\n",
            "          86     0.3913    0.9000    0.5455        10\n",
            "          87     0.9000    0.9000    0.9000        10\n",
            "          88     1.0000    0.3000    0.4615        10\n",
            "          89     0.6667    0.8000    0.7273        10\n",
            "          90     0.8182    0.9000    0.8571        10\n",
            "          91     0.4667    0.7000    0.5600        10\n",
            "          92     0.9000    0.9000    0.9000        10\n",
            "          93     0.5714    0.8000    0.6667        10\n",
            "          94     0.4444    0.8000    0.5714        10\n",
            "          95     1.0000    0.4000    0.5714        10\n",
            "          96     0.6667    0.4000    0.5000        10\n",
            "          97     1.0000    0.9000    0.9474        10\n",
            "          98     0.7500    0.6000    0.6667        10\n",
            "          99     1.0000    1.0000    1.0000        10\n",
            "\n",
            "    accuracy                         0.6720      1000\n",
            "   macro avg     0.7203    0.6720    0.6622      1000\n",
            "weighted avg     0.7203    0.6720    0.6622      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 4000\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10869.83 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 237.39 MB (2.2% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 4000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 4000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t30.0s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t4000 features in original data used to generate 4000 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 6.19 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 31.18s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 568.82s of the 568.7s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 568.48s of the 568.37s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 568.16s of the 568.06s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6203\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t125.5s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t6.29s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 434.99s of the 434.89s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 843. Best iteration is:\n",
            "\t[756]\tvalid_set's multi_logloss: 3.2718\tvalid_set's f1_weighted: 0.303881\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3039\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t440.05s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t6.59s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -15.68s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6203\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.47s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 616.83s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0      NeuralNetFastAI   0.620286       6.294241  125.496701                6.294241         125.496701            1       True          1\n",
            "1  WeightedEnsemble_L2   0.620286       6.295681  125.969325                0.001439           0.472623            2       True          3\n",
            "2           LightGBMXT   0.303881       6.592954  440.051677                6.592954         440.051677            1       True          2\n",
            "Number of models trained: 3\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 4000 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6246862712772789\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6246862712772789,\n",
            "    \"accuracy\": 0.638,\n",
            "    \"balanced_accuracy\": 0.638,\n",
            "    \"mcc\": 0.6350044395633424\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6429    0.9000    0.7500        10\n",
            "           1     0.9000    0.9000    0.9000        10\n",
            "           2     0.6667    1.0000    0.8000        10\n",
            "           3     0.5882    1.0000    0.7407        10\n",
            "           4     0.8000    0.8000    0.8000        10\n",
            "           5     0.7500    0.3000    0.4286        10\n",
            "           6     0.7500    0.9000    0.8182        10\n",
            "           7     0.2000    0.2000    0.2000        10\n",
            "           8     0.7143    0.5000    0.5882        10\n",
            "           9     1.0000    0.9000    0.9474        10\n",
            "          10     0.7500    0.6000    0.6667        10\n",
            "          11     0.4091    0.9000    0.5625        10\n",
            "          12     1.0000    0.2000    0.3333        10\n",
            "          13     0.8571    0.6000    0.7059        10\n",
            "          14     1.0000    0.3000    0.4615        10\n",
            "          15     0.5000    0.4000    0.4444        10\n",
            "          16     0.8333    1.0000    0.9091        10\n",
            "          17     0.3636    0.4000    0.3810        10\n",
            "          18     0.2000    0.1000    0.1333        10\n",
            "          19     0.6154    0.8000    0.6957        10\n",
            "          20     0.5294    0.9000    0.6667        10\n",
            "          21     0.6923    0.9000    0.7826        10\n",
            "          22     0.3333    0.4000    0.3636        10\n",
            "          23     0.6667    0.4000    0.5000        10\n",
            "          24     0.5000    0.5000    0.5000        10\n",
            "          25     0.6667    0.6000    0.6316        10\n",
            "          26     0.3333    0.1000    0.1538        10\n",
            "          27     0.4000    0.6000    0.4800        10\n",
            "          28     0.6667    0.4000    0.5000        10\n",
            "          29     0.0714    0.1000    0.0833        10\n",
            "          30     0.8182    0.9000    0.8571        10\n",
            "          31     0.5625    0.9000    0.6923        10\n",
            "          32     0.7000    0.7000    0.7000        10\n",
            "          33     0.9091    1.0000    0.9524        10\n",
            "          34     0.9091    1.0000    0.9524        10\n",
            "          35     0.4444    0.4000    0.4211        10\n",
            "          36     0.8333    1.0000    0.9091        10\n",
            "          37     0.3333    0.4000    0.3636        10\n",
            "          38     0.2000    0.1000    0.1333        10\n",
            "          39     0.8333    0.5000    0.6250        10\n",
            "          40     0.8889    0.8000    0.8421        10\n",
            "          41     0.4444    0.4000    0.4211        10\n",
            "          42     1.0000    0.3000    0.4615        10\n",
            "          43     0.8750    0.7000    0.7778        10\n",
            "          44     0.8333    0.5000    0.6250        10\n",
            "          45     0.8333    0.5000    0.6250        10\n",
            "          46     0.6429    0.9000    0.7500        10\n",
            "          47     1.0000    0.9000    0.9474        10\n",
            "          48     0.7500    0.6000    0.6667        10\n",
            "          49     1.0000    0.6000    0.7500        10\n",
            "          50     0.8333    1.0000    0.9091        10\n",
            "          51     0.5882    1.0000    0.7407        10\n",
            "          52     1.0000    0.8000    0.8889        10\n",
            "          53     0.5000    0.6000    0.5455        10\n",
            "          54     0.6667    0.6000    0.6316        10\n",
            "          55     0.6923    0.9000    0.7826        10\n",
            "          56     1.0000    0.5000    0.6667        10\n",
            "          57     0.6364    0.7000    0.6667        10\n",
            "          58     1.0000    0.3000    0.4615        10\n",
            "          59     0.6000    0.6000    0.6000        10\n",
            "          60     0.2353    0.4000    0.2963        10\n",
            "          61     0.8000    0.4000    0.5333        10\n",
            "          62     1.0000    0.2000    0.3333        10\n",
            "          63     0.3333    0.3000    0.3158        10\n",
            "          64     0.4167    0.5000    0.4545        10\n",
            "          65     0.8182    0.9000    0.8571        10\n",
            "          66     0.6923    0.9000    0.7826        10\n",
            "          67     0.4000    1.0000    0.5714        10\n",
            "          68     0.6667    0.6000    0.6316        10\n",
            "          69     0.8889    0.8000    0.8421        10\n",
            "          70     0.3333    0.7000    0.4516        10\n",
            "          71     0.7778    0.7000    0.7368        10\n",
            "          72     0.5000    1.0000    0.6667        10\n",
            "          73     0.8333    1.0000    0.9091        10\n",
            "          74     0.9091    1.0000    0.9524        10\n",
            "          75     0.2500    0.1000    0.1429        10\n",
            "          76     0.8000    0.4000    0.5333        10\n",
            "          77     0.5333    0.8000    0.6400        10\n",
            "          78     0.8333    0.5000    0.6250        10\n",
            "          79     0.8889    0.8000    0.8421        10\n",
            "          80     0.8000    0.4000    0.5333        10\n",
            "          81     0.2000    0.1000    0.1333        10\n",
            "          82     0.8571    0.6000    0.7059        10\n",
            "          83     1.0000    0.8000    0.8889        10\n",
            "          84     0.9091    1.0000    0.9524        10\n",
            "          85     1.0000    0.8000    0.8889        10\n",
            "          86     0.2667    0.4000    0.3200        10\n",
            "          87     0.7692    1.0000    0.8696        10\n",
            "          88     0.5556    0.5000    0.5263        10\n",
            "          89     0.8571    0.6000    0.7059        10\n",
            "          90     0.7143    1.0000    0.8333        10\n",
            "          91     0.5714    0.8000    0.6667        10\n",
            "          92     0.6923    0.9000    0.7826        10\n",
            "          93     0.7778    0.7000    0.7368        10\n",
            "          94     0.4444    0.8000    0.5714        10\n",
            "          95     1.0000    0.5000    0.6667        10\n",
            "          96     0.5000    0.1000    0.1667        10\n",
            "          97     1.0000    0.4000    0.5714        10\n",
            "          98     0.6364    0.7000    0.6667        10\n",
            "          99     0.7692    1.0000    0.8696        10\n",
            "\n",
            "    accuracy                         0.6380      1000\n",
            "   macro avg     0.6796    0.6380    0.6247      1000\n",
            "weighted avg     0.6796    0.6380    0.6247      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 4500\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10812.0 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 266.63 MB (2.5% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 4500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 4500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t34.8s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t4500 features in original data used to generate 4500 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 6.96 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 36.36s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 563.64s of the 563.51s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 563.24s of the 563.09s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 562.82s of the 562.68s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5897\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t137.06s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t7.78s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 416.53s of the 416.38s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 757. Best iteration is:\n",
            "\t[661]\tvalid_set's multi_logloss: 3.23208\tvalid_set's f1_weighted: 0.306429\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3064\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t421.68s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t8.16s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -17.54s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.5921\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.47s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 618.61s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.592063      15.945079  559.203015                0.002761           0.468743            2       True          3\n",
            "1      NeuralNetFastAI   0.589683       7.780744  137.056403                7.780744         137.056403            1       True          1\n",
            "2           LightGBMXT   0.306429       8.161575  421.677869                8.161575         421.677869            1       True          2\n",
            "Number of models trained: 3\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 4500 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6280429069337806\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6280429069337806,\n",
            "    \"accuracy\": 0.642,\n",
            "    \"balanced_accuracy\": 0.6419999999999999,\n",
            "    \"mcc\": 0.6390820457361361\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7273    0.8000    0.7619        10\n",
            "           1     0.7000    0.7000    0.7000        10\n",
            "           2     0.9091    1.0000    0.9524        10\n",
            "           3     0.6250    1.0000    0.7692        10\n",
            "           4     0.4737    0.9000    0.6207        10\n",
            "           5     0.7500    0.3000    0.4286        10\n",
            "           6     0.8889    0.8000    0.8421        10\n",
            "           7     0.2000    0.1000    0.1333        10\n",
            "           8     0.7778    0.7000    0.7368        10\n",
            "           9     1.0000    0.9000    0.9474        10\n",
            "          10     0.7778    0.7000    0.7368        10\n",
            "          11     0.6154    0.8000    0.6957        10\n",
            "          12     1.0000    0.1000    0.1818        10\n",
            "          13     0.7143    0.5000    0.5882        10\n",
            "          14     1.0000    0.3000    0.4615        10\n",
            "          15     0.1667    0.1000    0.1250        10\n",
            "          16     0.6250    1.0000    0.7692        10\n",
            "          17     0.4167    0.5000    0.4545        10\n",
            "          18     0.3333    0.1000    0.1538        10\n",
            "          19     0.8750    0.7000    0.7778        10\n",
            "          20     0.5714    0.8000    0.6667        10\n",
            "          21     0.7500    0.6000    0.6667        10\n",
            "          22     0.8333    0.5000    0.6250        10\n",
            "          23     0.6667    0.4000    0.5000        10\n",
            "          24     0.6667    0.4000    0.5000        10\n",
            "          25     0.8889    0.8000    0.8421        10\n",
            "          26     0.6667    0.4000    0.5000        10\n",
            "          27     0.4615    0.6000    0.5217        10\n",
            "          28     0.9000    0.9000    0.9000        10\n",
            "          29     0.2500    0.2000    0.2222        10\n",
            "          30     0.5263    1.0000    0.6897        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     1.0000    0.5000    0.6667        10\n",
            "          33     0.6429    0.9000    0.7500        10\n",
            "          34     0.8889    0.8000    0.8421        10\n",
            "          35     0.1667    0.2000    0.1818        10\n",
            "          36     0.5556    1.0000    0.7143        10\n",
            "          37     0.4000    0.4000    0.4000        10\n",
            "          38     0.3889    0.7000    0.5000        10\n",
            "          39     0.6364    0.7000    0.6667        10\n",
            "          40     1.0000    0.8000    0.8889        10\n",
            "          41     0.5714    0.4000    0.4706        10\n",
            "          42     0.5385    0.7000    0.6087        10\n",
            "          43     0.5833    0.7000    0.6364        10\n",
            "          44     0.7500    0.3000    0.4286        10\n",
            "          45     0.6364    0.7000    0.6667        10\n",
            "          46     0.6667    0.8000    0.7273        10\n",
            "          47     0.9091    1.0000    0.9524        10\n",
            "          48     0.6667    0.6000    0.6316        10\n",
            "          49     1.0000    0.6000    0.7500        10\n",
            "          50     0.6000    0.9000    0.7200        10\n",
            "          51     0.5263    1.0000    0.6897        10\n",
            "          52     1.0000    0.7000    0.8235        10\n",
            "          53     0.8182    0.9000    0.8571        10\n",
            "          54     0.5385    0.7000    0.6087        10\n",
            "          55     0.4444    0.8000    0.5714        10\n",
            "          56     0.5455    0.6000    0.5714        10\n",
            "          57     0.8750    0.7000    0.7778        10\n",
            "          58     1.0000    0.2000    0.3333        10\n",
            "          59     1.0000    0.4000    0.5714        10\n",
            "          60     0.3333    0.6000    0.4286        10\n",
            "          61     0.7143    0.5000    0.5882        10\n",
            "          62     0.2000    0.2000    0.2000        10\n",
            "          63     0.4667    0.7000    0.5600        10\n",
            "          64     0.7500    0.3000    0.4286        10\n",
            "          65     0.6923    0.9000    0.7826        10\n",
            "          66     0.8000    0.8000    0.8000        10\n",
            "          67     1.0000    0.8000    0.8889        10\n",
            "          68     0.6667    0.6000    0.6316        10\n",
            "          69     0.8750    0.7000    0.7778        10\n",
            "          70     0.3913    0.9000    0.5455        10\n",
            "          71     0.5455    0.6000    0.5714        10\n",
            "          72     0.3600    0.9000    0.5143        10\n",
            "          73     0.8889    0.8000    0.8421        10\n",
            "          74     0.8333    1.0000    0.9091        10\n",
            "          75     0.5000    0.2000    0.2857        10\n",
            "          76     0.6667    0.4000    0.5000        10\n",
            "          77     0.8571    0.6000    0.7059        10\n",
            "          78     0.6250    0.5000    0.5556        10\n",
            "          79     0.6667    0.8000    0.7273        10\n",
            "          80     0.4286    0.6000    0.5000        10\n",
            "          81     0.4000    0.2000    0.2667        10\n",
            "          82     0.8000    0.4000    0.5333        10\n",
            "          83     1.0000    0.7000    0.8235        10\n",
            "          84     1.0000    1.0000    1.0000        10\n",
            "          85     0.8750    0.7000    0.7778        10\n",
            "          86     0.5714    0.4000    0.4706        10\n",
            "          87     0.6250    1.0000    0.7692        10\n",
            "          88     1.0000    0.2000    0.3333        10\n",
            "          89     0.5714    0.8000    0.6667        10\n",
            "          90     0.8333    1.0000    0.9091        10\n",
            "          91     0.7500    0.3000    0.4286        10\n",
            "          92     0.5294    0.9000    0.6667        10\n",
            "          93     0.8571    0.6000    0.7059        10\n",
            "          94     0.7778    0.7000    0.7368        10\n",
            "          95     0.6154    0.8000    0.6957        10\n",
            "          96     0.5000    0.2000    0.2857        10\n",
            "          97     0.5714    0.8000    0.6667        10\n",
            "          98     1.0000    0.9000    0.9474        10\n",
            "          99     0.9091    1.0000    0.9524        10\n",
            "\n",
            "    accuracy                         0.6420      1000\n",
            "   macro avg     0.6855    0.6420    0.6280      1000\n",
            "weighted avg     0.6855    0.6420    0.6280      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 5000\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10815.61 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 295.83 MB (2.7% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 5000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 5000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t39.4s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t5000 features in original data used to generate 5000 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 7.73 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 41.08s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 558.92s of the 558.75s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 558.42s of the 558.26s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 557.94s of the 557.78s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6401\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t151.48s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t9.38s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 395.48s of the 395.31s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3266\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t341.38s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t10.98s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBM ... Training model for up to 41.76s of the 41.58s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 25. Best iteration is:\n",
            "\t[23]\tvalid_set's multi_logloss: 3.46845\tvalid_set's f1_weighted: 0.207056\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.2071\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t44.31s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t9.04s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -13.95s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6564\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.62s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 615.35s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.656429      18.425701  196.398978                0.001529           0.617354            2       True          4\n",
            "1      NeuralNetFastAI   0.640095       9.384404  151.476074                9.384404         151.476074            1       True          1\n",
            "2           LightGBMXT   0.326595      10.975286  341.383774               10.975286         341.383774            1       True          2\n",
            "3             LightGBM   0.207056       9.039768   44.305550                9.039768          44.305550            1       True          3\n",
            "Number of models trained: 4\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 5000 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6551202814062709\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6551202814062709,\n",
            "    \"accuracy\": 0.663,\n",
            "    \"balanced_accuracy\": 0.6630000000000001,\n",
            "    \"mcc\": 0.6602077688571958\n",
            "}\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5000    0.9000    0.6429        10\n",
            "           1     0.6154    0.8000    0.6957        10\n",
            "           2     0.6667    1.0000    0.8000        10\n",
            "           3     0.8889    0.8000    0.8421        10\n",
            "           4     0.8889    0.8000    0.8421        10\n",
            "           5     0.6923    0.9000    0.7826        10\n",
            "           6     0.9000    0.9000    0.9000        10\n",
            "           7     0.8000    0.4000    0.5333        10\n",
            "           8     1.0000    0.6000    0.7500        10\n",
            "           9     0.8333    1.0000    0.9091        10\n",
            "          10     0.6364    0.7000    0.6667        10\n",
            "          11     0.9000    0.9000    0.9000        10\n",
            "          12     1.0000    0.1000    0.1818        10\n",
            "          13     0.8889    0.8000    0.8421        10\n",
            "          14     0.7500    0.6000    0.6667        10\n",
            "          15     0.4286    0.3000    0.3529        10\n",
            "          16     0.7692    1.0000    0.8696        10\n",
            "          17     0.5000    0.6000    0.5455        10\n",
            "          18     0.6000    0.3000    0.4000        10\n",
            "          19     0.7500    0.9000    0.8182        10\n",
            "          20     0.6364    0.7000    0.6667        10\n",
            "          21     0.8333    0.5000    0.6250        10\n",
            "          22     0.6250    0.5000    0.5556        10\n",
            "          23     0.5000    0.6000    0.5455        10\n",
            "          24     0.5714    0.4000    0.4706        10\n",
            "          25     0.8889    0.8000    0.8421        10\n",
            "          26     0.6000    0.9000    0.7200        10\n",
            "          27     0.4286    0.6000    0.5000        10\n",
            "          28     1.0000    0.6000    0.7500        10\n",
            "          29     0.1250    0.1000    0.1111        10\n",
            "          30     0.9000    0.9000    0.9000        10\n",
            "          31     0.9000    0.9000    0.9000        10\n",
            "          32     0.6000    0.6000    0.6000        10\n",
            "          33     1.0000    1.0000    1.0000        10\n",
            "          34     0.6000    0.9000    0.7200        10\n",
            "          35     0.2000    0.2000    0.2000        10\n",
            "          36     0.8333    1.0000    0.9091        10\n",
            "          37     0.6667    0.4000    0.5000        10\n",
            "          38     0.2381    0.5000    0.3226        10\n",
            "          39     0.5556    0.5000    0.5263        10\n",
            "          40     0.7692    1.0000    0.8696        10\n",
            "          41     0.6667    0.4000    0.5000        10\n",
            "          42     0.7500    0.6000    0.6667        10\n",
            "          43     0.8750    0.7000    0.7778        10\n",
            "          44     1.0000    0.5000    0.6667        10\n",
            "          45     0.6000    0.6000    0.6000        10\n",
            "          46     0.5882    1.0000    0.7407        10\n",
            "          47     0.7692    1.0000    0.8696        10\n",
            "          48     0.6154    0.8000    0.6957        10\n",
            "          49     0.8000    0.8000    0.8000        10\n",
            "          50     0.9000    0.9000    0.9000        10\n",
            "          51     0.5294    0.9000    0.6667        10\n",
            "          52     0.7692    1.0000    0.8696        10\n",
            "          53     0.6000    0.9000    0.7200        10\n",
            "          54     1.0000    0.6000    0.7500        10\n",
            "          55     0.6364    0.7000    0.6667        10\n",
            "          56     0.7778    0.7000    0.7368        10\n",
            "          57     0.7000    0.7000    0.7000        10\n",
            "          58     0.0000    0.0000    0.0000        10\n",
            "          59     0.7778    0.7000    0.7368        10\n",
            "          60     0.2143    0.6000    0.3158        10\n",
            "          61     0.5000    0.6000    0.5455        10\n",
            "          62     0.7143    0.5000    0.5882        10\n",
            "          63     0.5833    0.7000    0.6364        10\n",
            "          64     0.7500    0.3000    0.4286        10\n",
            "          65     0.8000    0.8000    0.8000        10\n",
            "          66     0.8333    0.5000    0.6250        10\n",
            "          67     0.6250    0.5000    0.5556        10\n",
            "          68     0.6000    0.6000    0.6000        10\n",
            "          69     0.6667    0.4000    0.5000        10\n",
            "          70     0.4444    0.8000    0.5714        10\n",
            "          71     1.0000    0.5000    0.6667        10\n",
            "          72     0.7000    0.7000    0.7000        10\n",
            "          73     1.0000    0.8000    0.8889        10\n",
            "          74     0.9091    1.0000    0.9524        10\n",
            "          75     0.3000    0.3000    0.3000        10\n",
            "          76     0.6667    0.4000    0.5000        10\n",
            "          77     0.5714    0.4000    0.4706        10\n",
            "          78     0.5714    0.4000    0.4706        10\n",
            "          79     1.0000    0.7000    0.8235        10\n",
            "          80     0.3333    0.1000    0.1538        10\n",
            "          81     0.5714    0.4000    0.4706        10\n",
            "          82     0.6667    0.4000    0.5000        10\n",
            "          83     1.0000    0.8000    0.8889        10\n",
            "          84     1.0000    1.0000    1.0000        10\n",
            "          85     0.7273    0.8000    0.7619        10\n",
            "          86     0.2800    0.7000    0.4000        10\n",
            "          87     0.9000    0.9000    0.9000        10\n",
            "          88     1.0000    0.3000    0.4615        10\n",
            "          89     0.7500    0.6000    0.6667        10\n",
            "          90     0.6923    0.9000    0.7826        10\n",
            "          91     0.4545    0.5000    0.4762        10\n",
            "          92     0.7500    0.9000    0.8182        10\n",
            "          93     0.7273    0.8000    0.7619        10\n",
            "          94     0.6364    0.7000    0.6667        10\n",
            "          95     0.5000    0.7000    0.5833        10\n",
            "          96     0.4286    0.3000    0.3529        10\n",
            "          97     1.0000    0.8000    0.8889        10\n",
            "          98     0.9000    0.9000    0.9000        10\n",
            "          99     0.9000    0.9000    0.9000        10\n",
            "\n",
            "    accuracy                         0.6630      1000\n",
            "   macro avg     0.6950    0.6630    0.6551      1000\n",
            "weighted avg     0.6950    0.6630    0.6551      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 5500\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10813.33 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 325.0 MB (3.0% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 5500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 5500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t44.2s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t5500 features in original data used to generate 5500 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 8.49 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 45.82s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 554.18s of the 553.98s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 553.59s of the 553.39s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 553.0s of the 552.8s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6205\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t178.24s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t10.94s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 362.03s of the 361.84s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3415\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t339.73s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t10.85s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBM ... Training model for up to 10.1s of the 9.9s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
            "\t[1]\tvalid_set's multi_logloss: 4.384\tvalid_set's f1_weighted: 0.0285672\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0286\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t25.87s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t10.83s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -29.09s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.634\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.61s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 630.55s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.633984      21.797675  518.584720                0.001514           0.614864            2       True          4\n",
            "1      NeuralNetFastAI   0.620460      10.944692  178.241163               10.944692         178.241163            1       True          1\n",
            "2           LightGBMXT   0.341500      10.851468  339.728693               10.851468         339.728693            1       True          2\n",
            "3             LightGBM   0.028567      10.827493   25.872866               10.827493          25.872866            1       True          3\n",
            "Number of models trained: 4\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 5500 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.655981006829799\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.655981006829799,\n",
            "    \"accuracy\": 0.665,\n",
            "    \"balanced_accuracy\": 0.665,\n",
            "    \"mcc\": 0.6622003596251675\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8000    0.8000    0.8000        10\n",
            "           1     0.7500    0.9000    0.8182        10\n",
            "           2     0.8333    1.0000    0.9091        10\n",
            "           3     0.6250    1.0000    0.7692        10\n",
            "           4     0.8571    0.6000    0.7059        10\n",
            "           5     0.2941    0.5000    0.3704        10\n",
            "           6     0.8333    1.0000    0.9091        10\n",
            "           7     0.6667    0.4000    0.5000        10\n",
            "           8     0.7500    0.6000    0.6667        10\n",
            "           9     0.9091    1.0000    0.9524        10\n",
            "          10     0.8333    0.5000    0.6250        10\n",
            "          11     0.7692    1.0000    0.8696        10\n",
            "          12     1.0000    0.4000    0.5714        10\n",
            "          13     0.7000    0.7000    0.7000        10\n",
            "          14     1.0000    0.4000    0.5714        10\n",
            "          15     0.4000    0.4000    0.4000        10\n",
            "          16     0.9091    1.0000    0.9524        10\n",
            "          17     0.5833    0.7000    0.6364        10\n",
            "          18     0.6667    0.2000    0.3077        10\n",
            "          19     0.6923    0.9000    0.7826        10\n",
            "          20     0.7273    0.8000    0.7619        10\n",
            "          21     0.8571    0.6000    0.7059        10\n",
            "          22     0.7500    0.6000    0.6667        10\n",
            "          23     0.4000    0.6000    0.4800        10\n",
            "          24     0.7500    0.3000    0.4286        10\n",
            "          25     0.8889    0.8000    0.8421        10\n",
            "          26     1.0000    0.8000    0.8889        10\n",
            "          27     0.2500    0.2000    0.2222        10\n",
            "          28     0.8750    0.7000    0.7778        10\n",
            "          29     0.2500    0.3000    0.2727        10\n",
            "          30     0.5625    0.9000    0.6923        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     1.0000    0.8000    0.8889        10\n",
            "          33     0.9000    0.9000    0.9000        10\n",
            "          34     0.5263    1.0000    0.6897        10\n",
            "          35     0.3500    0.7000    0.4667        10\n",
            "          36     0.8333    1.0000    0.9091        10\n",
            "          37     0.5000    0.2000    0.2857        10\n",
            "          38     0.3571    0.5000    0.4167        10\n",
            "          39     0.7500    0.3000    0.4286        10\n",
            "          40     0.7500    0.9000    0.8182        10\n",
            "          41     0.8000    0.4000    0.5333        10\n",
            "          42     0.8571    0.6000    0.7059        10\n",
            "          43     0.8000    0.8000    0.8000        10\n",
            "          44     0.7500    0.6000    0.6667        10\n",
            "          45     0.4706    0.8000    0.5926        10\n",
            "          46     0.6667    0.8000    0.7273        10\n",
            "          47     1.0000    1.0000    1.0000        10\n",
            "          48     0.6667    0.6000    0.6316        10\n",
            "          49     1.0000    0.6000    0.7500        10\n",
            "          50     0.5263    1.0000    0.6897        10\n",
            "          51     0.6250    1.0000    0.7692        10\n",
            "          52     0.9000    0.9000    0.9000        10\n",
            "          53     0.7778    0.7000    0.7368        10\n",
            "          54     0.6364    0.7000    0.6667        10\n",
            "          55     0.7500    0.9000    0.8182        10\n",
            "          56     0.6364    0.7000    0.6667        10\n",
            "          57     0.7143    0.5000    0.5882        10\n",
            "          58     1.0000    0.1000    0.1818        10\n",
            "          59     0.6667    0.6000    0.6316        10\n",
            "          60     0.2941    0.5000    0.3704        10\n",
            "          61     0.5714    0.4000    0.4706        10\n",
            "          62     1.0000    0.4000    0.5714        10\n",
            "          63     0.5333    0.8000    0.6400        10\n",
            "          64     0.5556    0.5000    0.5263        10\n",
            "          65     0.8750    0.7000    0.7778        10\n",
            "          66     0.7143    0.5000    0.5882        10\n",
            "          67     0.7143    1.0000    0.8333        10\n",
            "          68     0.3636    0.8000    0.5000        10\n",
            "          69     0.7500    0.3000    0.4286        10\n",
            "          70     0.5714    0.8000    0.6667        10\n",
            "          71     0.7778    0.7000    0.7368        10\n",
            "          72     1.0000    0.3000    0.4615        10\n",
            "          73     1.0000    0.9000    0.9474        10\n",
            "          74     0.7143    1.0000    0.8333        10\n",
            "          75     0.6667    0.2000    0.3077        10\n",
            "          76     1.0000    0.5000    0.6667        10\n",
            "          77     0.7143    0.5000    0.5882        10\n",
            "          78     0.3750    0.6000    0.4615        10\n",
            "          79     0.6923    0.9000    0.7826        10\n",
            "          80     0.3636    0.4000    0.3810        10\n",
            "          81     0.5000    0.3000    0.3750        10\n",
            "          82     1.0000    0.5000    0.6667        10\n",
            "          83     1.0000    0.8000    0.8889        10\n",
            "          84     0.9091    1.0000    0.9524        10\n",
            "          85     0.5833    0.7000    0.6364        10\n",
            "          86     0.2500    0.3000    0.2727        10\n",
            "          87     1.0000    0.9000    0.9474        10\n",
            "          88     0.6000    0.3000    0.4000        10\n",
            "          89     0.6667    1.0000    0.8000        10\n",
            "          90     0.7500    0.9000    0.8182        10\n",
            "          91     0.6000    0.6000    0.6000        10\n",
            "          92     0.6000    0.9000    0.7200        10\n",
            "          93     0.6364    0.7000    0.6667        10\n",
            "          94     1.0000    0.7000    0.8235        10\n",
            "          95     0.5000    0.5000    0.5000        10\n",
            "          96     0.7500    0.3000    0.4286        10\n",
            "          97     0.5833    0.7000    0.6364        10\n",
            "          98     0.7273    0.8000    0.7619        10\n",
            "          99     0.8000    0.8000    0.8000        10\n",
            "\n",
            "    accuracy                         0.6650      1000\n",
            "   macro avg     0.7110    0.6650    0.6560      1000\n",
            "weighted avg     0.7110    0.6650    0.6560      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 6000\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10815.5 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 354.14 MB (3.3% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 6000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 6000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t52.7s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t6000 features in original data used to generate 6000 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 9.22 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 54.77s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 545.23s of the 544.98s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 544.53s of the 544.29s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 543.84s of the 543.6s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6223\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t179.54s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t15.9s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 345.84s of the 345.6s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 543. Best iteration is:\n",
            "\t[331]\tvalid_set's multi_logloss: 3.18789\tvalid_set's f1_weighted: 0.3\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t350.54s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t12.47s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -21.73s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6309\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.47s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 623.11s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.630905      28.372572  530.542604                0.001581           0.465556            2       True          3\n",
            "1      NeuralNetFastAI   0.622286      15.902552  179.541266               15.902552         179.541266            1       True          1\n",
            "2           LightGBMXT   0.300000      12.468439  350.535782               12.468439         350.535782            1       True          2\n",
            "Number of models trained: 3\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 6000 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6705634600076616\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6705634600076616,\n",
            "    \"accuracy\": 0.683,\n",
            "    \"balanced_accuracy\": 0.6830000000000002,\n",
            "    \"mcc\": 0.6803018646555659\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6923    0.9000    0.7826        10\n",
            "           1     0.6923    0.9000    0.7826        10\n",
            "           2     0.7692    1.0000    0.8696        10\n",
            "           3     0.7692    1.0000    0.8696        10\n",
            "           4     0.9000    0.9000    0.9000        10\n",
            "           5     0.6667    0.6000    0.6316        10\n",
            "           6     0.9000    0.9000    0.9000        10\n",
            "           7     1.0000    0.3000    0.4615        10\n",
            "           8     0.8571    0.6000    0.7059        10\n",
            "           9     1.0000    0.9000    0.9474        10\n",
            "          10     0.8750    0.7000    0.7778        10\n",
            "          11     0.6667    1.0000    0.8000        10\n",
            "          12     1.0000    0.1000    0.1818        10\n",
            "          13     0.6250    0.5000    0.5556        10\n",
            "          14     1.0000    0.6000    0.7500        10\n",
            "          15     1.0000    0.4000    0.5714        10\n",
            "          16     0.7143    1.0000    0.8333        10\n",
            "          17     1.0000    0.3000    0.4615        10\n",
            "          18     0.6667    0.2000    0.3077        10\n",
            "          19     0.8182    0.9000    0.8571        10\n",
            "          20     0.5000    0.8000    0.6154        10\n",
            "          21     0.5385    0.7000    0.6087        10\n",
            "          22     0.6364    0.7000    0.6667        10\n",
            "          23     0.5000    0.6000    0.5455        10\n",
            "          24     1.0000    0.3000    0.4615        10\n",
            "          25     0.7143    0.5000    0.5882        10\n",
            "          26     0.8571    0.6000    0.7059        10\n",
            "          27     0.4545    0.5000    0.4762        10\n",
            "          28     0.8750    0.7000    0.7778        10\n",
            "          29     0.2222    0.4000    0.2857        10\n",
            "          30     0.5625    0.9000    0.6923        10\n",
            "          31     1.0000    1.0000    1.0000        10\n",
            "          32     0.5833    0.7000    0.6364        10\n",
            "          33     1.0000    0.9000    0.9474        10\n",
            "          34     0.8182    0.9000    0.8571        10\n",
            "          35     1.0000    0.2000    0.3333        10\n",
            "          36     0.7143    1.0000    0.8333        10\n",
            "          37     0.3333    0.2000    0.2500        10\n",
            "          38     0.3889    0.7000    0.5000        10\n",
            "          39     0.6667    0.6000    0.6316        10\n",
            "          40     0.8182    0.9000    0.8571        10\n",
            "          41     0.5000    0.3000    0.3750        10\n",
            "          42     0.7778    0.7000    0.7368        10\n",
            "          43     0.8889    0.8000    0.8421        10\n",
            "          44     0.4444    0.4000    0.4211        10\n",
            "          45     0.7778    0.7000    0.7368        10\n",
            "          46     0.5625    0.9000    0.6923        10\n",
            "          47     1.0000    1.0000    1.0000        10\n",
            "          48     0.6923    0.9000    0.7826        10\n",
            "          49     0.8182    0.9000    0.8571        10\n",
            "          50     0.7143    1.0000    0.8333        10\n",
            "          51     0.6667    1.0000    0.8000        10\n",
            "          52     0.9000    0.9000    0.9000        10\n",
            "          53     1.0000    0.6000    0.7500        10\n",
            "          54     0.7273    0.8000    0.7619        10\n",
            "          55     0.4737    0.9000    0.6207        10\n",
            "          56     0.8000    0.8000    0.8000        10\n",
            "          57     0.7143    0.5000    0.5882        10\n",
            "          58     1.0000    0.1000    0.1818        10\n",
            "          59     1.0000    0.4000    0.5714        10\n",
            "          60     0.2222    0.4000    0.2857        10\n",
            "          61     0.8333    0.5000    0.6250        10\n",
            "          62     0.2941    0.5000    0.3704        10\n",
            "          63     0.5833    0.7000    0.6364        10\n",
            "          64     0.7500    0.6000    0.6667        10\n",
            "          65     0.9000    0.9000    0.9000        10\n",
            "          66     0.7273    0.8000    0.7619        10\n",
            "          67     1.0000    0.9000    0.9474        10\n",
            "          68     0.7000    0.7000    0.7000        10\n",
            "          69     0.6667    0.6000    0.6316        10\n",
            "          70     0.5455    0.6000    0.5714        10\n",
            "          71     0.6364    0.7000    0.6667        10\n",
            "          72     0.7692    1.0000    0.8696        10\n",
            "          73     0.8000    0.8000    0.8000        10\n",
            "          74     0.6667    1.0000    0.8000        10\n",
            "          75     0.2500    0.2000    0.2222        10\n",
            "          76     0.2500    0.3000    0.2727        10\n",
            "          77     0.8571    0.6000    0.7059        10\n",
            "          78     0.6667    0.6000    0.6316        10\n",
            "          79     0.7273    0.8000    0.7619        10\n",
            "          80     0.5000    0.3000    0.3750        10\n",
            "          81     0.4286    0.3000    0.3529        10\n",
            "          82     0.8000    0.4000    0.5333        10\n",
            "          83     0.8889    0.8000    0.8421        10\n",
            "          84     0.9091    1.0000    0.9524        10\n",
            "          85     0.6923    0.9000    0.7826        10\n",
            "          86     0.5000    0.5000    0.5000        10\n",
            "          87     0.7692    1.0000    0.8696        10\n",
            "          88     0.4000    0.4000    0.4000        10\n",
            "          89     1.0000    0.8000    0.8889        10\n",
            "          90     0.7692    1.0000    0.8696        10\n",
            "          91     0.5000    0.7000    0.5833        10\n",
            "          92     0.8182    0.9000    0.8571        10\n",
            "          93     0.7000    0.7000    0.7000        10\n",
            "          94     0.7000    0.7000    0.7000        10\n",
            "          95     0.5833    0.7000    0.6364        10\n",
            "          96     1.0000    0.2000    0.3333        10\n",
            "          97     0.6667    0.8000    0.7273        10\n",
            "          98     0.8182    0.9000    0.8571        10\n",
            "          99     1.0000    1.0000    1.0000        10\n",
            "\n",
            "    accuracy                         0.6830      1000\n",
            "   macro avg     0.7254    0.6830    0.6706      1000\n",
            "weighted avg     0.7254    0.6830    0.6706      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 6500\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10834.52 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 383.26 MB (3.5% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 6500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 6500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t54.8s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t6500 features in original data used to generate 6500 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 9.97 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 56.98s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 543.02s of the 542.73s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 542.18s of the 541.91s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 541.36s of the 541.08s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6099\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t190.2s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t15.01s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 334.32s of the 334.05s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 510. Best iteration is:\n",
            "\t[429]\tvalid_set's multi_logloss: 3.12955\tvalid_set's f1_weighted: 0.319286\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3193\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t339.84s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t17.75s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -28.38s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6251\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.47s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 630.08s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.625143      32.765819  530.501473                0.001674           0.465931            2       True          3\n",
            "1      NeuralNetFastAI   0.609857      15.010486  190.195242               15.010486         190.195242            1       True          1\n",
            "2           LightGBMXT   0.319286      17.753658  339.840301               17.753658         339.840301            1       True          2\n",
            "Number of models trained: 3\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 6500 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.619781001699116\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.619781001699116,\n",
            "    \"accuracy\": 0.635,\n",
            "    \"balanced_accuracy\": 0.6350000000000001,\n",
            "    \"mcc\": 0.6317861908096722\n",
            "}\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5714    0.8000    0.6667        10\n",
            "           1     0.5333    0.8000    0.6400        10\n",
            "           2     0.7143    1.0000    0.8333        10\n",
            "           3     0.7143    1.0000    0.8333        10\n",
            "           4     0.5833    0.7000    0.6364        10\n",
            "           5     0.4545    0.5000    0.4762        10\n",
            "           6     1.0000    0.9000    0.9474        10\n",
            "           7     0.3333    0.1000    0.1538        10\n",
            "           8     0.9000    0.9000    0.9000        10\n",
            "           9     1.0000    1.0000    1.0000        10\n",
            "          10     1.0000    0.3000    0.4615        10\n",
            "          11     1.0000    1.0000    1.0000        10\n",
            "          12     1.0000    0.1000    0.1818        10\n",
            "          13     0.7000    0.7000    0.7000        10\n",
            "          14     1.0000    0.3000    0.4615        10\n",
            "          15     0.1538    0.2000    0.1739        10\n",
            "          16     0.8333    1.0000    0.9091        10\n",
            "          17     0.6250    0.5000    0.5556        10\n",
            "          18     0.3333    0.2000    0.2500        10\n",
            "          19     0.6923    0.9000    0.7826        10\n",
            "          20     0.6667    0.8000    0.7273        10\n",
            "          21     0.6154    0.8000    0.6957        10\n",
            "          22     0.4667    0.7000    0.5600        10\n",
            "          23     0.4000    0.6000    0.4800        10\n",
            "          24     0.6000    0.3000    0.4000        10\n",
            "          25     0.7273    0.8000    0.7619        10\n",
            "          26     0.5000    0.3000    0.3750        10\n",
            "          27     0.5455    0.6000    0.5714        10\n",
            "          28     0.7778    0.7000    0.7368        10\n",
            "          29     0.1667    0.2000    0.1818        10\n",
            "          30     0.8000    0.8000    0.8000        10\n",
            "          31     0.9000    0.9000    0.9000        10\n",
            "          32     0.5000    0.4000    0.4444        10\n",
            "          33     0.8333    1.0000    0.9091        10\n",
            "          34     0.6250    1.0000    0.7692        10\n",
            "          35     0.5000    0.2000    0.2857        10\n",
            "          36     0.6250    1.0000    0.7692        10\n",
            "          37     0.6000    0.3000    0.4000        10\n",
            "          38     0.3750    0.3000    0.3333        10\n",
            "          39     0.6667    0.2000    0.3077        10\n",
            "          40     0.7273    0.8000    0.7619        10\n",
            "          41     0.5000    0.3000    0.3750        10\n",
            "          42     0.7500    0.3000    0.4286        10\n",
            "          43     0.8000    0.8000    0.8000        10\n",
            "          44     0.6250    0.5000    0.5556        10\n",
            "          45     0.7000    0.7000    0.7000        10\n",
            "          46     0.7500    0.9000    0.8182        10\n",
            "          47     0.9091    1.0000    0.9524        10\n",
            "          48     0.7000    0.7000    0.7000        10\n",
            "          49     0.8889    0.8000    0.8421        10\n",
            "          50     0.5882    1.0000    0.7407        10\n",
            "          51     0.6923    0.9000    0.7826        10\n",
            "          52     0.7273    0.8000    0.7619        10\n",
            "          53     0.8000    0.8000    0.8000        10\n",
            "          54     0.5455    0.6000    0.5714        10\n",
            "          55     0.6364    0.7000    0.6667        10\n",
            "          56     0.5455    0.6000    0.5714        10\n",
            "          57     0.8750    0.7000    0.7778        10\n",
            "          58     0.0000    0.0000    0.0000        10\n",
            "          59     0.8000    0.4000    0.5333        10\n",
            "          60     0.1765    0.3000    0.2222        10\n",
            "          61     0.5000    0.3000    0.3750        10\n",
            "          62     0.4167    0.5000    0.4545        10\n",
            "          63     0.6667    0.8000    0.7273        10\n",
            "          64     0.3158    0.6000    0.4138        10\n",
            "          65     0.8889    0.8000    0.8421        10\n",
            "          66     0.5333    0.8000    0.6400        10\n",
            "          67     0.6923    0.9000    0.7826        10\n",
            "          68     0.4615    0.6000    0.5217        10\n",
            "          69     1.0000    0.5000    0.6667        10\n",
            "          70     0.4000    0.6000    0.4800        10\n",
            "          71     0.7778    0.7000    0.7368        10\n",
            "          72     0.5385    0.7000    0.6087        10\n",
            "          73     0.9000    0.9000    0.9000        10\n",
            "          74     0.9091    1.0000    0.9524        10\n",
            "          75     0.1818    0.2000    0.1905        10\n",
            "          76     0.2727    0.3000    0.2857        10\n",
            "          77     1.0000    0.5000    0.6667        10\n",
            "          78     0.3636    0.4000    0.3810        10\n",
            "          79     0.7000    0.7000    0.7000        10\n",
            "          80     0.4000    0.6000    0.4800        10\n",
            "          81     0.3636    0.4000    0.3810        10\n",
            "          82     0.8571    0.6000    0.7059        10\n",
            "          83     1.0000    0.7000    0.8235        10\n",
            "          84     1.0000    1.0000    1.0000        10\n",
            "          85     0.8000    0.8000    0.8000        10\n",
            "          86     0.2857    0.2000    0.2353        10\n",
            "          87     1.0000    1.0000    1.0000        10\n",
            "          88     1.0000    0.4000    0.5714        10\n",
            "          89     0.5263    1.0000    0.6897        10\n",
            "          90     0.8182    0.9000    0.8571        10\n",
            "          91     0.4000    0.4000    0.4000        10\n",
            "          92     0.6923    0.9000    0.7826        10\n",
            "          93     0.8750    0.7000    0.7778        10\n",
            "          94     0.6154    0.8000    0.6957        10\n",
            "          95     0.5000    0.6000    0.5455        10\n",
            "          96     1.0000    0.1000    0.1818        10\n",
            "          97     0.7500    0.6000    0.6667        10\n",
            "          98     0.8750    0.7000    0.7778        10\n",
            "          99     1.0000    0.9000    0.9474        10\n",
            "\n",
            "    accuracy                         0.6350      1000\n",
            "   macro avg     0.6605    0.6350    0.6198      1000\n",
            "weighted avg     0.6605    0.6350    0.6198      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 7000\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10842.42 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 412.32 MB (3.8% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 7000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 7000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t63.8s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t7000 features in original data used to generate 7000 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 10.7 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 66.56s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 533.44s of the 533.12s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 532.49s of the 532.17s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 531.54s of the 531.22s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6261\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t217.71s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t17.42s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 292.7s of the 292.09s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 419. Best iteration is:\n",
            "\t[302]\tvalid_set's multi_logloss: 3.10478\tvalid_set's f1_weighted: 0.322071\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3221\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t299.04s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t16.54s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -28.64s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6497\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.44s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 630.16s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.649714      33.962323  517.192431                0.002605           0.443556            2       True          3\n",
            "1      NeuralNetFastAI   0.626095      17.422872  217.713225               17.422872         217.713225            1       True          1\n",
            "2           LightGBMXT   0.322071      16.536846  299.035651               16.536846         299.035651            1       True          2\n",
            "Number of models trained: 3\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 7000 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6458221094916369\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6458221094916369,\n",
            "    \"accuracy\": 0.662,\n",
            "    \"balanced_accuracy\": 0.662,\n",
            "    \"mcc\": 0.6591907271885665\n",
            "}\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5294    0.9000    0.6667        10\n",
            "           1     0.5625    0.9000    0.6923        10\n",
            "           2     0.7143    1.0000    0.8333        10\n",
            "           3     0.6250    1.0000    0.7692        10\n",
            "           4     0.8750    0.7000    0.7778        10\n",
            "           5     0.3043    0.7000    0.4242        10\n",
            "           6     0.8889    0.8000    0.8421        10\n",
            "           7     0.8571    0.6000    0.7059        10\n",
            "           8     0.8333    0.5000    0.6250        10\n",
            "           9     0.8333    1.0000    0.9091        10\n",
            "          10     1.0000    0.6000    0.7500        10\n",
            "          11     0.6667    1.0000    0.8000        10\n",
            "          12     1.0000    0.1000    0.1818        10\n",
            "          13     0.6667    0.8000    0.7273        10\n",
            "          14     0.6667    0.2000    0.3077        10\n",
            "          15     0.7143    0.5000    0.5882        10\n",
            "          16     1.0000    1.0000    1.0000        10\n",
            "          17     0.6250    0.5000    0.5556        10\n",
            "          18     0.6250    0.5000    0.5556        10\n",
            "          19     1.0000    0.8000    0.8889        10\n",
            "          20     0.6429    0.9000    0.7500        10\n",
            "          21     0.7500    0.6000    0.6667        10\n",
            "          22     1.0000    0.2000    0.3333        10\n",
            "          23     0.6000    0.3000    0.4000        10\n",
            "          24     0.5556    0.5000    0.5263        10\n",
            "          25     1.0000    0.6000    0.7500        10\n",
            "          26     0.5000    0.1000    0.1667        10\n",
            "          27     0.7000    0.7000    0.7000        10\n",
            "          28     0.6667    0.6000    0.6316        10\n",
            "          29     0.1667    0.1000    0.1250        10\n",
            "          30     0.5833    0.7000    0.6364        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     0.5714    0.8000    0.6667        10\n",
            "          33     0.9091    1.0000    0.9524        10\n",
            "          34     0.5625    0.9000    0.6923        10\n",
            "          35     0.5000    0.4000    0.4444        10\n",
            "          36     1.0000    1.0000    1.0000        10\n",
            "          37     0.5000    0.2000    0.2857        10\n",
            "          38     0.7778    0.7000    0.7368        10\n",
            "          39     0.4167    0.5000    0.4545        10\n",
            "          40     0.8000    0.8000    0.8000        10\n",
            "          41     0.8000    0.4000    0.5333        10\n",
            "          42     0.5556    0.5000    0.5263        10\n",
            "          43     0.8750    0.7000    0.7778        10\n",
            "          44     0.5714    0.4000    0.4706        10\n",
            "          45     0.7500    0.9000    0.8182        10\n",
            "          46     0.5000    0.8000    0.6154        10\n",
            "          47     0.9091    1.0000    0.9524        10\n",
            "          48     0.6429    0.9000    0.7500        10\n",
            "          49     0.8333    0.5000    0.6250        10\n",
            "          50     0.7143    1.0000    0.8333        10\n",
            "          51     0.6250    1.0000    0.7692        10\n",
            "          52     0.9091    1.0000    0.9524        10\n",
            "          53     0.6667    0.6000    0.6316        10\n",
            "          54     0.6154    0.8000    0.6957        10\n",
            "          55     0.4211    0.8000    0.5517        10\n",
            "          56     0.8000    0.8000    0.8000        10\n",
            "          57     0.8750    0.7000    0.7778        10\n",
            "          58     0.0000    0.0000    0.0000        10\n",
            "          59     0.8333    0.5000    0.6250        10\n",
            "          60     0.3333    0.3000    0.3158        10\n",
            "          61     0.5000    0.3000    0.3750        10\n",
            "          62     0.3750    0.3000    0.3333        10\n",
            "          63     0.7273    0.8000    0.7619        10\n",
            "          64     0.5556    0.5000    0.5263        10\n",
            "          65     0.9000    0.9000    0.9000        10\n",
            "          66     0.4706    0.8000    0.5926        10\n",
            "          67     0.6923    0.9000    0.7826        10\n",
            "          68     0.6364    0.7000    0.6667        10\n",
            "          69     0.5000    0.4000    0.4444        10\n",
            "          70     0.3182    0.7000    0.4375        10\n",
            "          71     0.8571    0.6000    0.7059        10\n",
            "          72     0.5294    0.9000    0.6667        10\n",
            "          73     0.6429    0.9000    0.7500        10\n",
            "          74     0.8333    1.0000    0.9091        10\n",
            "          75     0.4000    0.2000    0.2667        10\n",
            "          76     0.5000    0.2000    0.2857        10\n",
            "          77     1.0000    0.7000    0.8235        10\n",
            "          78     0.6364    0.7000    0.6667        10\n",
            "          79     0.6667    0.8000    0.7273        10\n",
            "          80     0.3158    0.6000    0.4138        10\n",
            "          81     0.5000    0.3000    0.3750        10\n",
            "          82     0.6667    0.6000    0.6316        10\n",
            "          83     1.0000    0.7000    0.8235        10\n",
            "          84     1.0000    1.0000    1.0000        10\n",
            "          85     0.5000    0.8000    0.6154        10\n",
            "          86     0.6667    0.2000    0.3077        10\n",
            "          87     0.9091    1.0000    0.9524        10\n",
            "          88     0.5556    0.5000    0.5263        10\n",
            "          89     0.5385    0.7000    0.6087        10\n",
            "          90     0.7778    0.7000    0.7368        10\n",
            "          91     0.4167    0.5000    0.4545        10\n",
            "          92     0.9000    0.9000    0.9000        10\n",
            "          93     0.8889    0.8000    0.8421        10\n",
            "          94     0.6000    0.6000    0.6000        10\n",
            "          95     0.5000    0.5000    0.5000        10\n",
            "          96     0.8889    0.8000    0.8421        10\n",
            "          97     0.7000    0.7000    0.7000        10\n",
            "          98     0.8889    0.8000    0.8421        10\n",
            "          99     1.0000    1.0000    1.0000        10\n",
            "\n",
            "    accuracy                         0.6620      1000\n",
            "   macro avg     0.6837    0.6620    0.6458      1000\n",
            "weighted avg     0.6837    0.6620    0.6458      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 7500\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10866.78 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 441.34 MB (4.1% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 7500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 7500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t67.6s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t7500 features in original data used to generate 7500 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 11.41 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 70.34s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 529.66s of the 529.29s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 528.55s of the 528.18s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 527.45s of the 527.07s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.599\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t236.43s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t19.06s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 269.47s of the 269.07s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 354. Best iteration is:\n",
            "\t[155]\tvalid_set's multi_logloss: 3.0425\tvalid_set's f1_weighted: 0.330929\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3309\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t274.71s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t22.13s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -33.07s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.606\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.53s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 634.73s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.606000      41.185508  511.671611                0.001759           0.529775            2       True          3\n",
            "1      NeuralNetFastAI   0.599048      19.057018  236.434550               19.057018         236.434550            1       True          1\n",
            "2           LightGBMXT   0.330929      22.126731  274.707285               22.126731         274.707285            1       True          2\n",
            "Number of models trained: 3\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 7500 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6212943006187361\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6212943006187361,\n",
            "    \"accuracy\": 0.636,\n",
            "    \"balanced_accuracy\": 0.6359999999999999,\n",
            "    \"mcc\": 0.6329776475150536\n",
            "}\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7500    0.6000    0.6667        10\n",
            "           1     0.5455    0.6000    0.5714        10\n",
            "           2     0.8333    1.0000    0.9091        10\n",
            "           3     0.7500    0.9000    0.8182        10\n",
            "           4     0.8182    0.9000    0.8571        10\n",
            "           5     0.8000    0.4000    0.5333        10\n",
            "           6     0.6923    0.9000    0.7826        10\n",
            "           7     0.0000    0.0000    0.0000        10\n",
            "           8     0.8333    0.5000    0.6250        10\n",
            "           9     0.9091    1.0000    0.9524        10\n",
            "          10     0.8889    0.8000    0.8421        10\n",
            "          11     0.5000    1.0000    0.6667        10\n",
            "          12     1.0000    0.1000    0.1818        10\n",
            "          13     0.5000    0.7000    0.5833        10\n",
            "          14     0.6667    0.2000    0.3077        10\n",
            "          15     0.5556    0.5000    0.5263        10\n",
            "          16     0.6667    0.8000    0.7273        10\n",
            "          17     0.3333    0.7000    0.4516        10\n",
            "          18     0.0000    0.0000    0.0000        10\n",
            "          19     0.7000    0.7000    0.7000        10\n",
            "          20     0.6364    0.7000    0.6667        10\n",
            "          21     0.4211    0.8000    0.5517        10\n",
            "          22     1.0000    0.5000    0.6667        10\n",
            "          23     0.2941    0.5000    0.3704        10\n",
            "          24     1.0000    0.3000    0.4615        10\n",
            "          25     0.8000    0.4000    0.5333        10\n",
            "          26     0.5455    0.6000    0.5714        10\n",
            "          27     0.4000    0.4000    0.4000        10\n",
            "          28     0.7778    0.7000    0.7368        10\n",
            "          29     0.3333    0.3000    0.3158        10\n",
            "          30     0.9000    0.9000    0.9000        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     0.8333    0.5000    0.6250        10\n",
            "          33     0.8889    0.8000    0.8421        10\n",
            "          34     0.4545    1.0000    0.6250        10\n",
            "          35     0.3636    0.4000    0.3810        10\n",
            "          36     0.8333    1.0000    0.9091        10\n",
            "          37     0.5000    0.5000    0.5000        10\n",
            "          38     0.1818    0.2000    0.1905        10\n",
            "          39     0.5714    0.4000    0.4706        10\n",
            "          40     0.5714    0.8000    0.6667        10\n",
            "          41     0.5714    0.4000    0.4706        10\n",
            "          42     1.0000    0.8000    0.8889        10\n",
            "          43     0.6667    0.8000    0.7273        10\n",
            "          44     0.8571    0.6000    0.7059        10\n",
            "          45     0.5714    0.8000    0.6667        10\n",
            "          46     0.5294    0.9000    0.6667        10\n",
            "          47     0.8333    1.0000    0.9091        10\n",
            "          48     0.8571    0.6000    0.7059        10\n",
            "          49     1.0000    0.9000    0.9474        10\n",
            "          50     0.8333    1.0000    0.9091        10\n",
            "          51     0.5556    1.0000    0.7143        10\n",
            "          52     1.0000    0.8000    0.8889        10\n",
            "          53     0.6667    0.4000    0.5000        10\n",
            "          54     0.6667    0.6000    0.6316        10\n",
            "          55     0.5000    0.9000    0.6429        10\n",
            "          56     0.5455    0.6000    0.5714        10\n",
            "          57     0.7500    0.6000    0.6667        10\n",
            "          58     0.0000    0.0000    0.0000        10\n",
            "          59     1.0000    0.4000    0.5714        10\n",
            "          60     0.5000    0.4000    0.4444        10\n",
            "          61     0.6667    0.4000    0.5000        10\n",
            "          62     0.6667    0.4000    0.5000        10\n",
            "          63     0.8182    0.9000    0.8571        10\n",
            "          64     0.6667    0.4000    0.5000        10\n",
            "          65     1.0000    0.9000    0.9474        10\n",
            "          66     0.3000    0.3000    0.3000        10\n",
            "          67     0.8182    0.9000    0.8571        10\n",
            "          68     0.7500    0.6000    0.6667        10\n",
            "          69     1.0000    0.6000    0.7500        10\n",
            "          70     0.3810    0.8000    0.5161        10\n",
            "          71     0.7143    1.0000    0.8333        10\n",
            "          72     0.4000    0.6000    0.4800        10\n",
            "          73     0.8182    0.9000    0.8571        10\n",
            "          74     0.6154    0.8000    0.6957        10\n",
            "          75     0.3333    0.2000    0.2500        10\n",
            "          76     0.6667    0.4000    0.5000        10\n",
            "          77     0.8750    0.7000    0.7778        10\n",
            "          78     0.5385    0.7000    0.6087        10\n",
            "          79     0.5833    0.7000    0.6364        10\n",
            "          80     0.0000    0.0000    0.0000        10\n",
            "          81     0.3636    0.4000    0.3810        10\n",
            "          82     0.6667    0.4000    0.5000        10\n",
            "          83     1.0000    0.6000    0.7500        10\n",
            "          84     0.7143    1.0000    0.8333        10\n",
            "          85     0.6250    1.0000    0.7692        10\n",
            "          86     0.3750    0.3000    0.3333        10\n",
            "          87     1.0000    0.9000    0.9474        10\n",
            "          88     0.7500    0.3000    0.4286        10\n",
            "          89     0.9091    1.0000    0.9524        10\n",
            "          90     0.9000    0.9000    0.9000        10\n",
            "          91     0.5000    0.5000    0.5000        10\n",
            "          92     0.8182    0.9000    0.8571        10\n",
            "          93     0.7778    0.7000    0.7368        10\n",
            "          94     0.5625    0.9000    0.6923        10\n",
            "          95     0.3333    0.7000    0.4516        10\n",
            "          96     1.0000    0.3000    0.4615        10\n",
            "          97     0.6923    0.9000    0.7826        10\n",
            "          98     1.0000    0.5000    0.6667        10\n",
            "          99     1.0000    0.8000    0.8889        10\n",
            "\n",
            "    accuracy                         0.6360      1000\n",
            "   macro avg     0.6655    0.6360    0.6213      1000\n",
            "weighted avg     0.6655    0.6360    0.6213      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 8000\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10871.62 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 470.36 MB (4.3% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 8000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 8000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t77.1s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t8000 features in original data used to generate 8000 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 12.12 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 80.08s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 519.92s of the 519.47s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 518.62s of the 518.18s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 517.34s of the 516.92s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6421\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t254.76s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t21.61s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 238.65s of the 238.22s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 285. Best iteration is:\n",
            "\t[193]\tvalid_set's multi_logloss: 3.09119\tvalid_set's f1_weighted: 0.3205\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3205\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t244.36s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t21.15s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -33.03s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.6514\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.47s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 634.74s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.651381      42.761281  499.590143                0.001492           0.473452            2       True          3\n",
            "1      NeuralNetFastAI   0.642095      21.614265  254.756213               21.614265         254.756213            1       True          1\n",
            "2           LightGBMXT   0.320500      21.145525  244.360478               21.145525         244.360478            1       True          2\n",
            "Number of models trained: 3\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 8000 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6442929309684897\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6442929309684897,\n",
            "    \"accuracy\": 0.656,\n",
            "    \"balanced_accuracy\": 0.6559999999999998,\n",
            "    \"mcc\": 0.6529626861805891\n",
            "}\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5714    0.8000    0.6667        10\n",
            "           1     0.6364    0.7000    0.6667        10\n",
            "           2     0.8333    1.0000    0.9091        10\n",
            "           3     0.6923    0.9000    0.7826        10\n",
            "           4     0.7500    0.9000    0.8182        10\n",
            "           5     0.5455    0.6000    0.5714        10\n",
            "           6     0.8182    0.9000    0.8571        10\n",
            "           7     0.6364    0.7000    0.6667        10\n",
            "           8     0.8750    0.7000    0.7778        10\n",
            "           9     1.0000    1.0000    1.0000        10\n",
            "          10     0.6000    0.6000    0.6000        10\n",
            "          11     0.5882    1.0000    0.7407        10\n",
            "          12     1.0000    0.2000    0.3333        10\n",
            "          13     0.5000    0.5000    0.5000        10\n",
            "          14     0.7143    0.5000    0.5882        10\n",
            "          15     0.7500    0.3000    0.4286        10\n",
            "          16     0.8333    1.0000    0.9091        10\n",
            "          17     0.4000    0.6000    0.4800        10\n",
            "          18     0.2500    0.3000    0.2727        10\n",
            "          19     0.7143    1.0000    0.8333        10\n",
            "          20     0.6667    0.8000    0.7273        10\n",
            "          21     0.6154    0.8000    0.6957        10\n",
            "          22     0.5000    0.7000    0.5833        10\n",
            "          23     0.5000    0.5000    0.5000        10\n",
            "          24     0.4000    0.2000    0.2667        10\n",
            "          25     0.7000    0.7000    0.7000        10\n",
            "          26     0.5833    0.7000    0.6364        10\n",
            "          27     0.6000    0.6000    0.6000        10\n",
            "          28     0.8750    0.7000    0.7778        10\n",
            "          29     0.1250    0.1000    0.1111        10\n",
            "          30     1.0000    0.8000    0.8889        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     0.7778    0.7000    0.7368        10\n",
            "          33     0.7273    0.8000    0.7619        10\n",
            "          34     0.7143    1.0000    0.8333        10\n",
            "          35     0.2500    0.4000    0.3077        10\n",
            "          36     0.8333    1.0000    0.9091        10\n",
            "          37     0.5000    0.3000    0.3750        10\n",
            "          38     0.4000    0.6000    0.4800        10\n",
            "          39     0.5000    0.3000    0.3750        10\n",
            "          40     0.7500    0.9000    0.8182        10\n",
            "          41     0.4000    0.4000    0.4000        10\n",
            "          42     1.0000    0.5000    0.6667        10\n",
            "          43     0.7500    0.6000    0.6667        10\n",
            "          44     0.5455    0.6000    0.5714        10\n",
            "          45     0.8571    0.6000    0.7059        10\n",
            "          46     0.6364    0.7000    0.6667        10\n",
            "          47     0.7692    1.0000    0.8696        10\n",
            "          48     0.6923    0.9000    0.7826        10\n",
            "          49     0.8750    0.7000    0.7778        10\n",
            "          50     0.8182    0.9000    0.8571        10\n",
            "          51     0.5556    1.0000    0.7143        10\n",
            "          52     1.0000    0.9000    0.9474        10\n",
            "          53     1.0000    0.3000    0.4615        10\n",
            "          54     1.0000    0.6000    0.7500        10\n",
            "          55     0.5000    0.7000    0.5833        10\n",
            "          56     0.6667    0.6000    0.6316        10\n",
            "          57     0.8889    0.8000    0.8421        10\n",
            "          58     0.0000    0.0000    0.0000        10\n",
            "          59     1.0000    0.5000    0.6667        10\n",
            "          60     0.3182    0.7000    0.4375        10\n",
            "          61     0.6364    0.7000    0.6667        10\n",
            "          62     0.6000    0.3000    0.4000        10\n",
            "          63     0.8750    0.7000    0.7778        10\n",
            "          64     0.6250    0.5000    0.5556        10\n",
            "          65     0.7500    0.9000    0.8182        10\n",
            "          66     0.5833    0.7000    0.6364        10\n",
            "          67     0.9000    0.9000    0.9000        10\n",
            "          68     0.5833    0.7000    0.6364        10\n",
            "          69     1.0000    0.5000    0.6667        10\n",
            "          70     0.6250    0.5000    0.5556        10\n",
            "          71     0.6000    0.6000    0.6000        10\n",
            "          72     0.3846    0.5000    0.4348        10\n",
            "          73     1.0000    0.9000    0.9474        10\n",
            "          74     0.6667    1.0000    0.8000        10\n",
            "          75     0.2000    0.1000    0.1333        10\n",
            "          76     0.6667    0.4000    0.5000        10\n",
            "          77     0.7778    0.7000    0.7368        10\n",
            "          78     0.4615    0.6000    0.5217        10\n",
            "          79     0.6154    0.8000    0.6957        10\n",
            "          80     0.1429    0.1000    0.1176        10\n",
            "          81     0.5455    0.6000    0.5714        10\n",
            "          82     0.8571    0.6000    0.7059        10\n",
            "          83     1.0000    0.8000    0.8889        10\n",
            "          84     1.0000    1.0000    1.0000        10\n",
            "          85     0.6429    0.9000    0.7500        10\n",
            "          86     0.3333    0.2000    0.2500        10\n",
            "          87     0.8333    1.0000    0.9091        10\n",
            "          88     1.0000    0.3000    0.4615        10\n",
            "          89     0.5833    0.7000    0.6364        10\n",
            "          90     0.7143    1.0000    0.8333        10\n",
            "          91     0.4118    0.7000    0.5185        10\n",
            "          92     0.9000    0.9000    0.9000        10\n",
            "          93     0.7143    0.5000    0.5882        10\n",
            "          94     0.7273    0.8000    0.7619        10\n",
            "          95     0.5455    0.6000    0.5714        10\n",
            "          96     0.7500    0.3000    0.4286        10\n",
            "          97     0.5556    0.5000    0.5263        10\n",
            "          98     0.8571    0.6000    0.7059        10\n",
            "          99     1.0000    0.8000    0.8889        10\n",
            "\n",
            "    accuracy                         0.6560      1000\n",
            "   macro avg     0.6766    0.6560    0.6443      1000\n",
            "weighted avg     0.6766    0.6560    0.6443      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 8500\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10875.05 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 499.36 MB (4.6% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 8500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 8500 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t80.1s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t8500 features in original data used to generate 8500 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 12.86 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 83.29s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 516.71s of the 516.21s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 515.24s of the 514.76s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 513.77s of the 513.28s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.595\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t268.84s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t24.24s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: LightGBMXT ... Training model for up to 217.88s of the 217.39s of remaining time.\n",
            "INFO:autogluon.tabular.models.lgb.callbacks:\tRan out of time, early stopping on iteration 239. Best iteration is:\n",
            "\t[212]\tvalid_set's multi_logloss: 3.10773\tvalid_set's f1_weighted: 0.301659\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.3017\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t224.11s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t23.45s\t = Validation runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the -36.89s of remaining time.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.596\t = Validation score   (f1_weighted)\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.47s\t = Training   runtime\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\t0.0s\t = Validation runtime\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon training complete, total runtime = 638.68s ... Best model: \"WeightedEnsemble_L2\"\n",
            "INFO:autogluon.tabular.predictor.predictor:TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"ag_models/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2   0.595952      47.692584  493.421978                0.002007           0.471973            2       True          3\n",
            "1      NeuralNetFastAI   0.595000      24.240225  268.838409               24.240225         268.838409            1       True          1\n",
            "2           LightGBMXT   0.301659      23.450352  224.111595               23.450352         224.111595            1       True          2\n",
            "Number of models trained: 3\n",
            "Types of models trained:\n",
            "{'LGBModel', 'WeightedEnsembleModel', 'NNFastAiTabularModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 8500 | ['0', '1', '2', '3', '4', ...]\n",
            "Plot summary of models saved to file: ag_models/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluation: f1_weighted on test data: 0.6415286344374108\n",
            "INFO:autogluon.tabular.learner.abstract_learner:Evaluations on test data:\n",
            "INFO:autogluon.tabular.learner.abstract_learner:{\n",
            "    \"f1_weighted\": 0.6415286344374108,\n",
            "    \"accuracy\": 0.658,\n",
            "    \"balanced_accuracy\": 0.6579999999999999,\n",
            "    \"mcc\": 0.6552301638071941\n",
            "}\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5000    0.8000    0.6154        10\n",
            "           1     0.9000    0.9000    0.9000        10\n",
            "           2     0.9091    1.0000    0.9524        10\n",
            "           3     1.0000    1.0000    1.0000        10\n",
            "           4     0.7000    0.7000    0.7000        10\n",
            "           5     0.5556    0.5000    0.5263        10\n",
            "           6     1.0000    0.9000    0.9474        10\n",
            "           7     0.7500    0.6000    0.6667        10\n",
            "           8     0.8750    0.7000    0.7778        10\n",
            "           9     1.0000    0.9000    0.9474        10\n",
            "          10     0.8333    0.5000    0.6250        10\n",
            "          11     0.6429    0.9000    0.7500        10\n",
            "          12     1.0000    0.1000    0.1818        10\n",
            "          13     0.7143    0.5000    0.5882        10\n",
            "          14     0.6000    0.6000    0.6000        10\n",
            "          15     0.2083    0.5000    0.2941        10\n",
            "          16     1.0000    1.0000    1.0000        10\n",
            "          17     0.8571    0.6000    0.7059        10\n",
            "          18     0.2727    0.3000    0.2857        10\n",
            "          19     0.7500    0.9000    0.8182        10\n",
            "          20     0.7273    0.8000    0.7619        10\n",
            "          21     0.7273    0.8000    0.7619        10\n",
            "          22     0.6000    0.6000    0.6000        10\n",
            "          23     0.2273    0.5000    0.3125        10\n",
            "          24     0.3750    0.3000    0.3333        10\n",
            "          25     0.8333    0.5000    0.6250        10\n",
            "          26     0.6667    0.6000    0.6316        10\n",
            "          27     1.0000    0.1000    0.1818        10\n",
            "          28     0.7273    0.8000    0.7619        10\n",
            "          29     0.5000    0.1000    0.1667        10\n",
            "          30     0.6154    0.8000    0.6957        10\n",
            "          31     1.0000    0.9000    0.9474        10\n",
            "          32     0.7500    0.6000    0.6667        10\n",
            "          33     0.8182    0.9000    0.8571        10\n",
            "          34     0.7143    1.0000    0.8333        10\n",
            "          35     0.5000    0.2000    0.2857        10\n",
            "          36     0.6667    1.0000    0.8000        10\n",
            "          37     1.0000    0.1000    0.1818        10\n",
            "          38     0.4000    0.6000    0.4800        10\n",
            "          39     0.6250    0.5000    0.5556        10\n",
            "          40     0.5000    0.8000    0.6154        10\n",
            "          41     0.5556    0.5000    0.5263        10\n",
            "          42     0.6667    0.6000    0.6316        10\n",
            "          43     0.7778    0.7000    0.7368        10\n",
            "          44     0.6667    0.4000    0.5000        10\n",
            "          45     0.6667    0.4000    0.5000        10\n",
            "          46     0.7273    0.8000    0.7619        10\n",
            "          47     0.8333    1.0000    0.9091        10\n",
            "          48     0.8000    0.8000    0.8000        10\n",
            "          49     0.8182    0.9000    0.8571        10\n",
            "          50     0.7692    1.0000    0.8696        10\n",
            "          51     0.6000    0.9000    0.7200        10\n",
            "          52     0.7500    0.9000    0.8182        10\n",
            "          53     0.7273    0.8000    0.7619        10\n",
            "          54     0.5385    0.7000    0.6087        10\n",
            "          55     0.5294    0.9000    0.6667        10\n",
            "          56     0.8571    0.6000    0.7059        10\n",
            "          57     0.6364    0.7000    0.6667        10\n",
            "          58     0.0000    0.0000    0.0000        10\n",
            "          59     0.5714    0.4000    0.4706        10\n",
            "          60     0.4286    0.6000    0.5000        10\n",
            "          61     1.0000    0.6000    0.7500        10\n",
            "          62     0.3333    0.2000    0.2500        10\n",
            "          63     0.8889    0.8000    0.8421        10\n",
            "          64     0.5833    0.7000    0.6364        10\n",
            "          65     0.6000    0.9000    0.7200        10\n",
            "          66     0.7778    0.7000    0.7368        10\n",
            "          67     0.8000    0.8000    0.8000        10\n",
            "          68     0.7778    0.7000    0.7368        10\n",
            "          69     0.5000    0.6000    0.5455        10\n",
            "          70     0.5385    0.7000    0.6087        10\n",
            "          71     0.8750    0.7000    0.7778        10\n",
            "          72     0.3200    0.8000    0.4571        10\n",
            "          73     0.7500    0.9000    0.8182        10\n",
            "          74     0.7500    0.9000    0.8182        10\n",
            "          75     0.3571    0.5000    0.4167        10\n",
            "          76     0.5000    0.8000    0.6154        10\n",
            "          77     0.8750    0.7000    0.7778        10\n",
            "          78     0.6000    0.6000    0.6000        10\n",
            "          79     0.8000    0.4000    0.5333        10\n",
            "          80     0.4706    0.8000    0.5926        10\n",
            "          81     0.5000    0.1000    0.1667        10\n",
            "          82     0.8571    0.6000    0.7059        10\n",
            "          83     1.0000    0.8000    0.8889        10\n",
            "          84     1.0000    1.0000    1.0000        10\n",
            "          85     1.0000    0.8000    0.8889        10\n",
            "          86     0.0000    0.0000    0.0000        10\n",
            "          87     1.0000    1.0000    1.0000        10\n",
            "          88     0.7500    0.3000    0.4286        10\n",
            "          89     0.5455    0.6000    0.5714        10\n",
            "          90     0.8182    0.9000    0.8571        10\n",
            "          91     0.6429    0.9000    0.7500        10\n",
            "          92     0.4737    0.9000    0.6207        10\n",
            "          93     0.7143    0.5000    0.5882        10\n",
            "          94     0.5714    0.8000    0.6667        10\n",
            "          95     0.7000    0.7000    0.7000        10\n",
            "          96     0.3333    0.1000    0.1538        10\n",
            "          97     0.7273    0.8000    0.7619        10\n",
            "          98     0.8000    0.4000    0.5333        10\n",
            "          99     1.0000    0.8000    0.8889        10\n",
            "\n",
            "    accuracy                         0.6580      1000\n",
            "   macro avg     0.6849    0.6580    0.6415      1000\n",
            "weighted avg     0.6849    0.6580    0.6415      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3641: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  self[k1] = value[k2]\n",
            "WARNING:autogluon.common.utils.utils:Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:Beginning AutoGluon training ... Time limit = 600s\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon will save models to \"ag_models/\"\n",
            "INFO:autogluon.tabular.learner.default_learner:AutoGluon Version:  0.5.2\n",
            "INFO:autogluon.tabular.learner.default_learner:Python Version:     3.7.15\n",
            "INFO:autogluon.tabular.learner.default_learner:Operating System:   Linux\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Rows:    1000\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Columns: 9000\n",
            "INFO:autogluon.tabular.learner.default_learner:Label Column: Label\n",
            "INFO:autogluon.tabular.learner.default_learner:Preprocessing data ...\n",
            "INFO:autogluon.tabular.learner.default_learner:Train Data Class Count: 100\n",
            "INFO:autogluon.tabular.learner.default_learner:Using Feature Generators to preprocess the data ...\n",
            "INFO:autogluon.features.generators.abstract:Fitting AutoMLPipelineFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tAvailable Memory:                    10890.56 MB\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Original)  Memory Usage: 528.36 MB (4.9% of available memory)\n",
            "INFO:autogluon.features.generators.abstract:\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "INFO:autogluon.features.generators.abstract:\tStage 1 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting AsTypeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 2 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting FillNaFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 3 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting CategoryFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tStage 4 Generators:\n",
            "INFO:autogluon.features.generators.abstract:\t\tFitting DropUniqueFeatureGenerator...\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in original data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('object', []) : 9000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "INFO:autogluon.common.features.feature_metadata:\t\t('category', []) : 9000 | ['0', '1', '2', '3', '4', ...]\n",
            "INFO:autogluon.features.generators.abstract:\t87.7s = Fit runtime\n",
            "INFO:autogluon.features.generators.abstract:\t9000 features in original data used to generate 9000 features in processed data.\n",
            "INFO:autogluon.features.generators.abstract:\tTrain Data (Processed) Memory Usage: 13.58 MB (0.1% of available memory)\n",
            "INFO:autogluon.tabular.learner.default_learner:Data preprocessing and feature engineering runtime = 91.09s ...\n",
            "Level 25:autogluon.core.trainer.abstract_trainer:AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
            "INFO:autogluon.core.trainer.abstract_trainer:\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "INFO:autogluon.tabular.trainer.auto_trainer:Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting 13 L1 models ...\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsUnif ... Training model for up to 508.91s of the 508.37s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsUnif... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: KNeighborsDist ... Training model for up to 507.27s of the 506.73s of remaining time.\n",
            "WARNING:autogluon.core.trainer.abstract_trainer:\tNo valid features to train KNeighborsDist... Skipping this model.\n",
            "INFO:autogluon.core.trainer.abstract_trainer:Fitting model: NeuralNetFastAI ... Training model for up to 505.64s of the 505.09s of remaining time.\n",
            "/usr/local/lib/python3.7/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py:345: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_train[LABEL] = pd.concat([y, y_val], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "result_txt = open(f\"result_NordVPN_cutting_size_CV1.txt\", 'w')\n",
        "\n",
        "for cutting_size in range(500, 10500, 500):\n",
        "  df_train = pd.read_csv(f'NordVPN_CV1_train.csv', )\n",
        "  df_test = pd.read_csv(f'NordVPN_CV1_test.csv', )\n",
        "\n",
        "  title_small = []\n",
        "\n",
        "  for i in range(cutting_size):\n",
        "    title_small.append(str(i))\n",
        "\n",
        "  df_train[title_small] = (df_train['Packet_Length_Sequence'].str.split(' ', expand=True)).drop(list(range(cutting_size, 10000)), axis='columns')\n",
        "  df_test[title_small] = (df_test['Packet_Length_Sequence'].str.split(' ', expand=True)).drop(list(range(cutting_size, 10000)), axis='columns')\n",
        "\n",
        "\n",
        "  df_train = df_train.drop(labels=['Packet_Length_Sequence'], axis='columns')   \n",
        "  df_test = df_test.drop(labels=['Packet_Length_Sequence'], axis='columns')   \n",
        "\n",
        "  save_path = 'ag_models/'\n",
        "\n",
        "  predictor = TabularPredictor(label=\"Label\", problem_type='multiclass', eval_metric='f1_weighted', path=save_path).fit(df_train, time_limit=600)\n",
        "\n",
        "  results = predictor.fit_summary(show_plot=True)\n",
        "\n",
        "  y_test = df_test['Label']    \n",
        "  df_test = df_test.drop(labels=['Label'], axis=1)   \n",
        "  y_pred = predictor.predict(df_test)\n",
        "  y_pred_prob = predictor.predict_proba(df_test)\n",
        "  perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
        "  labels_100 = [int(i) for i in range(100)]\n",
        "  print(classification_report(y_test, y_pred, labels=labels_100, digits=4))\n",
        "\n",
        "  result_txt.write(f\"Cutting_size : {cutting_size}\\n\")\n",
        "  result_txt.write(classification_report(y_test, y_pred, labels=labels_100, digits=4))\n",
        "\n",
        "result_txt.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}